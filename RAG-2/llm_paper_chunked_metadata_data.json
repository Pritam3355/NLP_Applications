{
    "document_metadata": {
        "filetype": "application/pdf",
        "document_length": "42",
        "file_directory": "/content",
        "filename": "llm_paper.pdf",
        "page_layout": {
            "width": 1654,
            "height": 2339,
            "system": "PixelSpace"
        },
        "last_timestamp": [
            "2025-07-31T13:28:15"
        ],
        "language": [
            "eng"
        ]
    },
    "1": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {
                "1": {
                    "4": {
                        "image_path": "/content/llm_paper_images/figure-1-4.jpg",
                        "image_heading": "a diagram of the different types of the different types of the different types of the different types of the different"
                    },
                    "5": {
                        "image_path": "/content/llm_paper_images/figure-1-5.jpg",
                        "image_heading": "a barcode with a black and white stripe"
                    },
                    "7": {
                        "image_path": "/content/llm_paper_images/figure-1-7.jpg",
                        "image_heading": "a pink background with a black and white stripe"
                    },
                    "1": {
                        "image_path": "/content/llm_paper_images/figure-1-1.jpg",
                        "image_heading": "the logo for the keye group"
                    },
                    "3": {
                        "image_path": "/content/llm_paper_images/figure-1-3.jpg",
                        "image_heading": "a barcode with a black and white stripe"
                    },
                    "8": {
                        "image_path": "/content/llm_paper_images/figure-1-8.jpg",
                        "image_heading": "a pink and black striped wallpaper"
                    },
                    "10": {
                        "image_path": "/content/llm_paper_images/figure-1-10.jpg",
                        "image_heading": "a white curtain with black stripes"
                    },
                    "11": {
                        "image_path": "/content/llm_paper_images/figure-1-11.jpg",
                        "image_heading": "a barcode with a black and white stripe"
                    },
                    "2": {
                        "image_path": "/content/llm_paper_images/figure-1-2.jpg",
                        "image_heading": "the logo for the company"
                    },
                    "6": {
                        "image_path": "/content/llm_paper_images/figure-1-6.jpg",
                        "image_heading": "a map of the world ' s major cities"
                    },
                    "9": {
                        "image_path": "/content/llm_paper_images/figure-1-9.jpg",
                        "image_heading": "a white and black door with a black handle"
                    }
                }
            },
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {
            "arXiv": [
                "i",
                "X",
                "r",
                "a"
            ],
            "O keye": [
                "July 3, 2025"
            ],
            "Kwai Keye-VL Technical Report": [
                "Keye Team, Kuaishou Group",
                "",
                "https://kwai-keye.github.io/",
                "https://huggingface.co/Kwai-Keye",
                "https://github.com/Kwai-Keye/Keye"
            ],
            "Abstract": [
                "While Multimodal Large Language Models (MLLMs) demonstrate remarkable capabili- ties on static images, they often fall short in comprehending dynamic, information-dense short-form videos, a dominant medium in todays digital landscape. To bridge this gap, we introduce Kwai Keye-VL, an 8-billion-parameter multimodal foundation model en- gineered for leading-edge performance in short-video understanding while maintaining robust general-purpose vision-language abilities. The development of Keye-VL rests on two core pillars: a massive, high-quality dataset exceeding 600 billion tokens with a strong emphasis on video, and an innovative training recipe. This recipe features a four- stage pre-training process for solid vision-language alignment, followed by a meticulous two-phase post-training process. The first post-training stage enhances foundational capabilities like instruction following, while the second phase focuses on stimulating advanced reasoning. In this second phase, a key innovation is our five-mode cold-start data mixture, which includes thinking, non-thinking, auto-think, think with image, and high-quality video data. This mixture teaches the model to decide when and how to reason. Subsequent reinforcement learning (RL) and alignment steps further enhance these reasoning capabilities and correct abnormal model behaviors, such as repetitive outputs. To validate our approach, we conduct extensive evaluations, showing that Keye-VL achieves state-of-the-art results on public video benchmarks and remains highly competitive on general image-based tasks (Figure 1). Furthermore, we develop and release the KC-MMBench, a new benchmark tailored for real-world short-video scenarios, where Keye-VL shows a significant advantage. Comprehensive human evalua- tions also confirm that our model provides a superior user experience compared to other leading models of a similar scale. This paper details the architecture, data construction strategy, and training methodology of Keye-VL, offering valuable insights for building the next generation of MLLMs for the video era."
            ],
            "Video": [
                "Understanding Benchmarks)",
                "+ Qwen2.5-VL-7B LongVideoBench (Public IntemVL-3-88 bier > Kwai Keye-VL-88 TempCompass VideoMMMU Short Video (Avg) Comment Collection Order 5 Hot Vid Short-Video Aggregation High Like",
                "",
                "|| HallusionBench General Ai2D wmstar Benchmarks RealWorld@a SimpievaA VLM are Blind MathVision. LogicvVista MathVista (OlympiadBench",
                "",
                "",
                "a"
            ],
            "Perception Benchmarks": [
                "",
                "5",
                "(KC-MMBench)"
            ],
            "Benchmarks": [],
            "Reasoning": [
                "Figure 1: Benchmark performance of Kwai Keye-VL: Keye-VL-8B establishes a new state-of-the-art among models of a similar scale, showing a clear lead in video-centric benchmarks (left) while maintaining competitive results on general perception and reasoning tasks (right), validating our training approach.",
                "1"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "Multimodal Large Language Models",
                "short-form videos",
                "foundation model",
                "dataset",
                "pre-training",
                "post-training",
                "video understanding",
                "vision-language abilities",
                "reinforcement learning"
            ],
            "page_summary": "This page introduces Kwai Keye-VL, an 8-billion-parameter multimodal foundation model designed for short-video understanding. It uses a four-stage pre-training process, followed by a two-phase post-training process, and reinforcement learning. The model achieves state-of-the-art results on public video benchmarks and remains highly competitive on general image-based tasks.",
            "page_number": "1"
        }
    },
    "2": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {
            "2": {
                "1": {
                    "table_path": "/content/llm_paper_images/table-2-1.jpg",
                    "table_heading": "",
                    "table_data": "Introduction\n\nModel Architecture\n\n2.1 Vision Encoder with Native-Resolution .. 2... 0.000000 cee eee\n\n2.2 VisualEncoding .. 11... ee\n\nPre-Training\n\n3.1 DataPipeline \u00a9... 12. ee\n3.1.1 Image Caption Data... 1... ee\nBal2: OCREVOA Data's oc ee stn e none e Gar ete HOON HALEN OR ee\n3.1.3 Grounding &Counting Data... 2. eee\n3.14. Interleaved Text-Image Data 2.26 cs wu woes Ce ee RR Re Rw ee\nSS! \u2018VideO'Datais < as zat 98 YSLEME ESIESS ROUGE ERASE EWE! E CH Ewe Owe\n\no2- \u2018raining Recipe, sme ge sme BE ARIE. SG SHER E AWE PHY PYRE BREE Be\n\nPost-Training\n\n4.1 No-Reasoning Training: Establishing Foundational Performance. ..............\n41.1 Step Li) Supervised Fine-Tuning s 2 aces cme ws cee we CREE ERE wE Bae\n4.1.2 Step 1.2: Mixed Preference Optimization... 6... 0.0.0.2... eee eee\n\n4.2 Reasoning Training: Core Breakthrough for Complex Cognition ...............\n42:1 Step ILInCol ColdsStatt . 2020. cmswe ememe eae me eee ee Eww\n422\u00b0 Step IL2: Mix-Mod\u00e9 RL : ses 23 Haws EW ews RE s CREE ERE wE Bae\n4.2.3 Step IL.3: Iterative Alignment .. 2... ee\n\nTraining Infrastructure\n\nEvaluation\n\n6.1 Zeto-shot Image Classificationof VII... 2 ecu cw eee em ee Ow ee ee\n\n6:2 Public Berichmarks \u00ab 2: 22 24 oe a Hs ER Ewe HESS EEE REE Rw RHE Bee\n\n6:3: Internal Benchmarks: -: 222 24 ges 4s oS ewe HERS EERE ER EE RO EH E Be\n63.1 Design Strategies and Core Principles <<. ce ews ch ees CH EME ERE wE Bas\n6.3.2 Evaluation Metrics and Baselines... 2... 0. ee ee\n6.3.3 EvaluationResults .. 2... 0... ee\n6.3.4 Analysis of Kwai Keye-VL's Limitations... 1... 2... ee ee eee\n\n6:4 Quantitative-Results; 2. cau \u00aba sm ens waste OME Se RE ew Ee we\n\nan\n\noA NNN N DTD SD\n\n10\n11\n11\n11\n11\n12\n13\n\n13\n\f"
                }
            }
        },
        "formula": [],
        "footer": [],
        "topic_content": {
            "Contents": [
                "2.1 Vision Encoder with Native-Resolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Visual Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1 Data Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.1 Image Caption Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.2 OCR &VQA Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.3 Grounding &Counting Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.4 Interleaved Text-Image Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.5 Video Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Training Recipe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1 No-Reasoning Training: Establishing Foundational Performance . . . . . . . . . . . . . . . 4.1.1 Step I.1: Supervised Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.2 Step I.2: Mixed Preference Optimization . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Reasoning Training: Core Breakthrough for Complex Cognition . . . . . . . . . . . . . . . 4.2.1 Step II.1: CoT Cold-Start . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.2 Step II.2: Mix-Mode RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.3 Step II.3: Iterative Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.1 Zero-shot Image Classification of ViT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2 Public Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3 Internal Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3.1 Design Strategies and Core Principles . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3.2 Evaluation Metrics and Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3.3 Evaluation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.3.4 Analysis of Kwai Keye-VLs Limitations . . . . . . . . . . . . . . . . . . . . . . . . . 6.4 Quantitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 5 6 6 6 6 7 7 7 7 8 9 9 10 11 11 11 11 12 13 13 14 14 14 16 16 19 19 20 21"
            ],
            "1 Introduction": [],
            "2 Model Architecture": [],
            "3 Pre-Training": [],
            "4 Post-Training": [],
            "5 Training Infrastructure": [],
            "6 Evaluation": [],
            "7 Discussion": [
                "7.1 Mutual Enhancement between Reasoning and Non-Reasoning Data . . . . . . . . . . . . .",
                "7.2 Performance Gain from RL Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
                "7.3 Analysis about Auto-Think Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
            ],
            "8 Conclusion and Future Work": [
                "2",
                "21",
                "21",
                "21",
                "22"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "machine learning model architecture",
                "pre-training and post-training",
                "data pipeline and data types",
                "training recipes and strategies",
                "evaluation and benchmarks",
                "performance analysis and future work"
            ],
            "page_summary": "This page discusses the architecture of a machine learning model, including pre-training and post-training stages. It covers the data pipeline, including various types of data used for training. The page also presents training recipes and strategies, as well as evaluation methods and benchmarks. Performance analysis and future work are also discussed.",
            "page_number": "2"
        }
    },
    "3": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {
            "3": {
                "2": {
                    "table_path": "/content/llm_paper_images/table-3-2.jpg",
                    "table_heading": "",
                    "table_data": "Strategies for Data Decontamination\n(AGL, Brestraiming) ov. 2c em eoe se Mak we KAT AK FAKE HR Ee me Re Rw wt 8\nAC2: Post Traitinigt.: + 0. sar eaic ama me Bate e AGH ean BR ASIN RE GHS We Baie On Bae\n\nConstruction of KC-MMbench\n\nCase Study\nC1 Modality \u00a9... 6 ee\n\nC.1.1 Pure TextCase 2... ee\nC12 Image Cases... 6. ee\n\nCue3 Video GaseS: sa.icas waisiee om ee See ee EE Ee ee ea Re BE\nC2. ThitkitigMede \u00ab2 css ewe we Ho ee CE Re Re ee Re Bae\n\nC21 Agentic Thinking Case =. us ee ews ew ewe Rew Ee OH ESE HE ewe EE Ewes\nC.2.2 Auto Thinking Cases... 1 ee\n\nAuthors (Alphabetical order)\n\n29\n29\n29\n\n29\n\n33\n33\n\n33\n34\n\n36\n38\n\n38\n39\n\n4]\n\f"
                }
            }
        },
        "formula": [],
        "footer": [],
        "topic_content": {
            "A Strategies for Data Decontamination": [],
            "B Construction of KC-MMbench": [],
            "C Case Study": [
                "D Authors (Alphabetical order)",
                "3",
                "29",
                "29",
                "29",
                "29",
                "33",
                "33",
                "33",
                "34",
                "36",
                "38",
                "38",
                "39",
                "41"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "Strategies for Data Decontamination",
                "Construction of KC-MMbench",
                "Case Study",
                "Authors"
            ],
            "page_summary": "The page discusses strategies for data decontamination, construction of KC-MMbench, and a case study. The authors' details are also provided in alphabetical order.",
            "page_number": "3"
        }
    },
    "4": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {
            "1 Introduction": [
                "In recent years, Large Language Models (LLMs) advance rapidly (Grattafiori et al. (2024); Abdin et al. (2024); Team (2025b); Wang et al. (2024c)), ushering in a new era of artificial intelligence with their powerful capabilities in understanding (FaceBook (2025); Team (2025a)), generating (Yang et al. (2025); Seed et al. (2025)), and reasoning with language (Guo et al. (2025a); Liu et al. (2024a)). This wave also propels the swift progress of Multimodal Large Language Models ( OpenAI (2025); Chen et al. (2024b;c); Hurst et al. (2024); Team et al. (2025b); Feng et al. (2024); Fu et al. (2025b); Han et al. (2024); Li et al. (2023); Luo et al. (2023); Guo et al. (2025b); Zhang et al. (2024b); Wu et al. (2024b)) (MLLMs), which extend these formidable language abilities to the visual domain, enabling them to perform complex tasks such as visual question answering (Li et al. (2024); Chen et al. (2024d)), detailed image captioning (Luo et al. (2024a); Rang et al. (2025); Li et al. (2025a)), object grounding (Bai et al. (2025); Ma et al. (2025)) and visual reasoning (OpenAI (2025); Su et al. (2025); Hu et al. (2025a)).",
                "Despite significant progress in understanding static images, a major challenge remains in comprehending video content (Shen et al. (2025); Lin et al. (2023)), which is more dynamic and information-dense (Luo et al. (2024c); Team et al. (2025a)). Short-form videos, in particular, become the primary medium for communication, entertainment, and commerce on platforms like Kuaishou( Zhou et al. (2025); Lu et al. (2025)). Understanding short videos is far more complex than recognizing individual objects (Li et al. (2024)); it requires a model to deeply comprehend the sequence of events, causal relationships, and the overall narrative. Furthermore, the model must integrate information from multiple sources, including video frames and audio (converted to text via Automatic Speech Recognition). Most existing multimodal models, primarily designed for handling combinations of single images and text, lack deep exploration for video tasks and thus often fail to capture the rich, contextual, and sequential information present in videos. To address this critical gap, we introduce Kwai Keye-VL, a meticulously engineered 8-billion-parameter multimodal foundation model. It achieves leading-edge performance in short-video understanding while also maintaining robust capabilities in general-purpose vision-language tasks (as shown in Figure 1). Our work is driven by a pressing need for a model that not only \"sees\" the world but also thinks about its dynamic patterns. This is crucial for enhancing user experience and enabling more intelligent applications in content creation, recommendation, and e-commerce on video-centric platforms.",
                "The development of Kwai Keye-VL rests upon several core techniques. First, we construct a large- scale and diverse dataset exceeding 600 billion tokens, with a special focus on high-quality video data. This data undergoes a rigorous processing pipeline, including filtering, re-captioning with advanced models to generate more precise descriptions, and frame-level annotation to ensure quality. Second, we design an innovative training methodology, which includes a four-stage pre-training process to build a solid foundation for vision-language alignment. Following pre-training, we further enhance Keye-VLs capabilities through a two-phase post-training process:",
                " Stage 1: optimizing foundational capabilities: We focus on improving the models basic performance in areas like instruction following. This is achieved through supervised fine-tuning (SFT) and mixed preference optimization (MPO) on high-quality data.",
                " Stage 2: stimulating and enhancing reasoning abilities: We begin by creating high-quality \"cold-start\" data containing five modes: conventional question-answering, long chain-of-thought, auto-reasoning decision, \"think with an image\" (e.g., generating code to process images), and high-quality video data. We train the model on a mix of these modes, teaching it to select the most appropriate response style. This allows it to think deeply for complex reasoning tasks while responding quickly to simple ones. Subsequently, we employ reinforcement learning to further strengthen its complex reasoning skills. Finally, we use the MPO algorithm for several rounds of iterative alignment to correct issues such as repetitive outputs and flawed logical reasoning.",
                "Throughout both pre-training and post-training, we perform rigorous data de-duplication. We compare our training data against general benchmark samples and remove those with high similarity. This process also reveals several public datasets currently implicated in significant data leakage, which we argue should be avoided in model training.",
                "To validate our approachs effectiveness, we first conduct extensive evaluations on multiple public bench- marks. As Figure 1 shows, Keye-VLs performance is highly competitive in general image understanding and reasoning, even reaching state-of-the-art level. In video understanding, Keye-VL-8B substantially out- performs previous state-of-the-art models across several benchmarks. To specifically address short-video understanding, we also develop and open-source the Kuaishou Community Multimodal Benchmark (KC-MMBench) and a comprehensive internal evaluation suite to rigorously assess the models capabili- ties in realistic, video-centric commercial application scenarios. On this benchmark, Keye-VL-8B also demonstrates a significant advantage, highlighting its value for commercial applications.",
                "4"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "Large Language Models (LLMs)",
                "Multimodal Large Language Models (MLLMs)",
                "artificial intelligence",
                "language understanding",
                "generating and reasoning with language",
                "video content comprehension",
                "short-form videos",
                "Kwai Keye-VL model",
                "video-centric platforms",
                "model training",
                "video understanding",
                "benchmarking",
                "commercial applications"
            ],
            "page_summary": "This page discusses the advancements in Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs), focusing on their language capabilities. A significant challenge remains in comprehending dynamic video content, particularly in short-form videos. To address this, the Kwai Keye-VL model, a meticulously engineered 8-billion-parameter multimodal foundation model, is introduced. This model achieves state-of-the-art performance in short-video understanding and general-purpose vision-language tasks. The model's development involves core techniques such as a large-scale dataset, innovative training methodology, and post-training processes to enhance reasoning abilities. Rigorous evaluations show Keye-VL's competitive performance in image understanding and substantially outperforms previous models in video understanding, particularly in short-video scenarios.",
            "page_number": "4"
        }
    },
    "5": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {
                "5": {
                    "15": {
                        "image_path": "/content/llm_paper_images/figure-5-15.jpg",
                        "image_heading": "a person is skiing down a hill"
                    },
                    "14": {
                        "image_path": "/content/llm_paper_images/figure-5-14.jpg",
                        "image_heading": "a person on a snowboard in the snow"
                    },
                    "18": {
                        "image_path": "/content/llm_paper_images/figure-5-18.jpg",
                        "image_heading": "a map of the british isles and islands"
                    },
                    "16": {
                        "image_path": "/content/llm_paper_images/figure-5-16.jpg",
                        "image_heading": "a person is snowboarding on a hill"
                    },
                    "17": {
                        "image_path": "/content/llm_paper_images/figure-5-17.jpg",
                        "image_heading": "a person skiing down a snowy mountain"
                    },
                    "13": {
                        "image_path": "/content/llm_paper_images/figure-5-13.jpg",
                        "image_heading": "a person is snowboarding on a snowy mountain"
                    },
                    "12": {
                        "image_path": "/content/llm_paper_images/figure-5-12.jpg",
                        "image_heading": "a keyboard keyboard with the key highlighted"
                    }
                }
            },
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {
            "16302 Tokens": [
                "There are 4 different visual information, the first"
            ],
            "Text Tokens": [
                "Projector (2x2 Patch Merge)",
                "a",
                " 4.26px> 4 2px",
                "Vision Encoder (Native-Resolution, 2D RoPE)",
                "FPS = 2/1/05",
                "<",
                "A Keye 140",
                "",
                "",
                "",
                "",
                "",
                "",
                "+",
                "Short-Video (18s)",
                ">"
            ],
            "Vision Tokens": [],
            "Video": [],
            "Image": [
                "Figure 2: The Kwai Keye-VL model architecture is based on the Qwen3-8B language model and incor- porates a vision encoder initialized from the open-source SigLIP. It supports native dynamic resolution, preserving the original aspect ratio of images by dividing each into a 14x14 patch sequence. A simple MLP layer then maps and merges the visual tokens. The model uses 3D RoPE for unified processing of text, image, and video information, establishing a one-to-one correspondence between position encoding and absolute time to ensure precise perception of temporal changes in video information.",
                "For a more granular assessment of Keye-VLs capabilities across various aspects and its real-world user experience, we construct an additional fine-grained internal benchmark. On this benchmark, we conduct a comprehensive and detailed human evaluation of similarly-sized models, including Keye-VL-8B, Qwen2.5-VL-7B, InternVL3-8B, and MiMo-VL-7B. The evaluation results indicate that our model delivers a superior user experience on both video and image-text tasks compared to these baselines. Concurrently, we present an in-depth analysis of Keye-VLs current limitations in fine-grained perception, temporal understanding, and high-level reasoning, outlining future directions for development.",
                "In summary, this paper provides a detailed account of the Kwai Keye-VLs architecture, data processing pipeline, training methodology, and comprehensive evaluation results, offering valuable insights for building the next generation of Multimodal Large Language Models for the video era."
            ],
            "2 Model Architecture": [
                "Figure 2 gives a high-level overview of our Keye-VL, which follows a classic MLLM architecture that includes three key components: a Vision Transformer (ViT), a MLP projector, and a language decoder. For ViT component, we apply the open-source SigLIP-400M-384-14 1 as our vision encoder to extract vision information. For LLM component, we employ the widely used Qwen3-8B as our language decoder, to provide the universal world semantic knowledge understanding capabilities. For the projector, we randomly initialize its parameters and fully pre-training it at the Stage 1. In the following sections, we provide our key upgrades, data pipeline and training recipes.",
                "1https://huggingface.co/google/siglip-so400m-patch14-384",
                "5",
                "SS",
                ">",
                "2px",
                "3192px"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "Multimodal Large Language Models",
                "Model Architecture",
                "Vision Encoder",
                "Language Decoder",
                "MLP Projector",
                "Training Methodology",
                "Evaluation"
            ],
            "page_summary": "This page discusses the Keye-VL model, a Multimodal Large Language Model with a classic architecture. It includes a Vision Transformer, a MLP projector, and a language decoder. The vision encoder, SigLIP-400M-384-14, is open-source. The language decoder, Qwen3-8B, provides universal world semantic knowledge understanding capabilities. The MLP projector is randomly initialized and fully pre-trained. The page also mentions the training methodology and comprehensive evaluation results.",
            "page_number": "5"
        }
    },
    "6": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {
            "2.1 Vision Encoder with Native-Resolution": [
                "In past years, many MLLMs efforts have adopted the well-trained fixed-resolution ViTs as their vision encoders, such as ViT-bigG (Cherti et al. (2023) ), SigLIP-400M (Zhai et al. (2023)) and others. However, unlike pre-trained CLIP-based ViTs (Radford et al. (2021) ) that only handle coarse-grained image-caption matching task during training, MLLMs often tackle various finer-grained generation tasks, existing a large gap between them. Therefore, we anticipate that our ViT will possess the following capabilities: during processing, images and videos maintain their structural integrity and all details are preserved.",
                "To this end, there are some pioneer MLLMs exploring native-resolution ViT in recent years, such as Qwen2.5-VL, Seed-VL-1.5, Kimi-VL, etc. In Keye-VL, we also implement a native-resolution ViT, to naturally process images at original resolution, avoiding some complex and redundant image splic- ing/splitting operations (e.g., MiniCPM2 (Yao et al. (2024))). Specifically, our ViT is initialized by the SigLIP-400M-384-14, a fixed-resolution variant with absolute learnable position embeddings to inject the spatial information. According to it, we first employ interpolation techniques to extend fixed- length learnable position embeddings into resolution-adaptive position embeddings, enabling our basic native-resolution modeling while preserving the pretrained workflow. Afterwards, to further enhance extrapolation capabilities for positional encoding along visual dimensions, we introduce 2D Rotary Position Embedding (RoPE) to strengthen the visual information modeling. In our trial experience, we observe that incorporating 2D RoPE significantly improves the models performance on high-resolution image. Finally, building upon the two types of position embeddings, we incorporate the NaViT packing with FlashAttention techniques to continue training our ViT across images with varying resolutions.",
                "During the ViT pre-training procedure, we optimize our native-resolution modifications via SigLIP loss function (the text tower is also from SigLIP-400M-384-14). We use the same distribution data as the downstream MLLM for training, including a total of 500B Tokens from open source data DataComp (Gadre et al. (2023)), LAION (Schuhmann et al. (2022)), CC12M (Changpinyo et al. (2021)), PD12M (Meyer et al. (2024)), COCO (Lin et al. (2014)) and other in-house data."
            ],
            "2.2 Visual Encoding": [
                "To guarantee that our language decoder can perceive enough visual signals to understand images and videos in detail, we leave sufficient token buffer for image and videos modeling.",
                "For images of different resolutions, we set the total number of tokens for each image to 16384, which can cover images with more than one million pixels and is sufficient to help the model to see the details of the image in most scenarios. For video modeling, we devise a dynamic resolution strategy that balances the maximum number of frames and the total number of tokens. In Keye-VL, we currently set the min/max token number per frame as 128/768, and the max vision token as 24576, this setting can automatically make trade-off between the breadth and depth of visual perception. Subsequently, based on the extracted frames, we re-calculate the FPS and ensure strict alignment in the time position in 3D RoPE dimensions during training (position +1 corresponds to +0.5 second in real world). Meanwhile, we are exploring other more efficient frame modeling techniques to ensure that more frames could feed to our LLM with acceptable computation."
            ],
            "3 Pre-Training": [
                "In this section, we first describe the construction of the pre-training dataset, followed by an overview of the overall training pipeline and configuration."
            ],
            "3.1 Data Pipeline": [
                "In our data construction pipeline, we have assembled a diverse, high-quality corpus with exceeding 600 billion tokens to support our models training, sourced from both public datasets and proprietary in-house data. Generally, our training data encompasses six primary categories: Image Caption, OCR & VQA, Grounding & Counting, Interleaved, Video Understanding and Pure Text data. To ensure these overall data quality, we have designed customized filtering mechanisms tailored to the characteristics of each data category. For large volumes of medium-quality data, we employ CLIP (Radford et al. (2021)) scores for preliminary filtering. For smaller amounts of high-quality data, we utilize open-source MLLMs as discriminators for data selection. Additionally, we also conduct rigorous image-based deduplication operation, to avoid the potential data leakage between our training corpus and evaluation benchmarks (Dixit et al. (2021)). Specifically, we identify highly similar images, then remove these near-duplicates",
                "6"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "Vision Encoder",
                "Native-Resolution",
                "MLLMs",
                "ViT",
                "Position Embeddings",
                "Pre-Training"
            ],
            "page_summary": "This page discusses the use of a native-resolution Vision Transformer (ViT) as a vision encoder in Multimodal Large Language Models (MLLMs). The authors describe how their ViT, initialized with a fixed-resolution variant, uses interpolation and 2D Rotary Position Embedding to process images at original resolution. The ViT is optimized during pre-training using a SigLIP loss function. The page also covers the data pipeline for the model's pre-training, which includes a diverse corpus of over 600 billion tokens from various sources.",
            "page_number": "6"
        }
    },
    "7": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {},
        "page_metadata": {
            "topics_detected": [],
            "page_summary": "",
            "page_number": "7"
        }
    },
    "8": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {
            "8": {
                "3": {
                    "table_path": "/content/llm_paper_images/table-8-3.jpg",
                    "table_heading": "",
                    "table_data": "Series pou\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nExample <|point_start | >[[x1, y1]]<| point_end | >\nDescription The [x1, y1] is the center point of queried objective.\nExample <|point_start | >[[x1, y1], [x2, y2]]<| point_end | >\nDescription Supporting multiple points for a single queried objective.\nExample <| object_ref_start | >obj< | object_ref_end | ><| point_start | >[[x1, y1]]<| point_end | >\nDescription The [x1, y1] is the center point of \u2018obj\u2019.\nbounding boxes\nExample <|box_start | >[[x1, y1, x2, y2]]<! box_end|>\nDescription The coordinates [x1, y1]/[x2, y2] denote the top-left and bottom-right point of box of queried objective.\nExample <|box_start | >[[x1, y1, x2, y2], [x3, y3, x4, y4]]< | box_end|>\nDescription Supporting multiple boxes for a single queried objective.\nExample <| object_ref_start | >obj< | object_ref_end | ><! box_start | >[[x1, y1, x2, y2]]< |! box_end | >\nDescription Detecting the \u2018obj\u2019 and its corresponding box.\nExample <|ocr_text_start | >text< | ocr_text_end | ><! box_start | >[[x1, y1, x2, y2]]< | box_end | >\n\nDescription\n\nIdentify the OCR results and its corresponding box.\n\n \n\f"
                }
            }
        },
        "formula": [],
        "footer": [],
        "topic_content": {
            "center points": [
                "Example <|point_start|>[[x1, y1]]<|point_end|> Description The [x1, y1] is the center point of queried objective. Example <|point_start|>[[x1, y1], [x2, y2]]<|point_end|> Description Supporting multiple points for a single queried objective. Example <|object_ref_start|>obj<|object_ref_end|><|point_start|>[[x1, y1]]<|point_end|> Description The [x1, y1] is the center point of obj. bounding boxes Example <|box_start|>[[x1, y1, x2, y2]]<|box_end|> Description The coordinates [x1, y1]/[x2, y2] denote the top-left and bottom-right point of box of queried objective. Example <|box_start|>[[x1, y1, x2, y2], [x3, y3, x4, y4]]<|box_end|> Description Supporting multiple boxes for a single queried objective. Example <|object_ref_start|>obj<|object_ref_end|><|box_start|>[[x1, y1, x2, y2]]<|box_end|> Description Detecting the obj and its corresponding box. Example <|ocr_text_start|>text<|ocr_text_end|><|box_start|>[[x1, y1, x2, y2]]<|box_end|> Description Identify the OCR results and its corresponding box."
            ],
            "polygons": [
                "Example <|object_ref_start|>obj<|object_ref_end|><|polygon_start|>[[[x1, y1], [x2, y2], [x3, y3]]]<|polygon_end|> Description The coordinates [x1, y1], [x2, y2], ... represent polygon vertices of obj, which arranged in clockwise order.",
                "Example <|ocr_text_start|>text<|ocr_text_end|><|polygon_start|>[[[x1, y1], [x2, y2], [x3, y3]]]<|polygon_end|> Description Supporting the OCR results.",
                "semantic signals in language model side, (3) Improved Generalization: The diverse and interleaved nature of the data strengthens the models ability to reason across modalities and generalize to unseen tasks. Besides the open-source interleaved data, we also build a large-scale in-house interleaved data generation pipeline. Specifically, we focus on the two type of raw rich-text documents processing, the academic PDF data and structured knowledge data, especially the Science, Technology, Engineering, and Mathematics (STEM) data. We collect a substantial amount of academic and knowledge-based PDF/structured data to render the text content into plain text format and insert the corresponding images at their original positions within the text. In such a process, we conduct rigorous data protection strategies to ensure high-quality outputs. Our pipeline includes: (1) Garbled character recognition: identifying and removing garbled characters, (2) Low-resolution/broken image filtering: ensuring image quality, (3) Text-image similarity validation: ensuring semantic alignment between interleaved image-text.",
                "3.1.5 Video Data",
                "As a short-video and live-streaming service provider, the video understanding ability is the most im- portant point of Kwai, such as understanding the video details, generating summaries, and expressing interesting implications. To reach the goal, our video data are collected from multiple sources, including diverse open-source datasets and a large-scale high-quality in-house video data. Based on these videos, we conduct the following key pipelines to guarantee our data quality:",
                " Interleaved video-ASR: For audio signals, we currently use speech-to-text tools (e.g., Qwen2.5- Omni (Xu et al. (2025a))) to recognize them, and then form a interleaved style to connect images and audio to our model.",
                " Video recaption: With (optional) ASR results, we next utilize diverse public MLLMs to generate its caption under different FPS setting, such as 0.5/1/2.",
                " Frame-level OCR annotation: In order to ensure that our model does not miss any details in each frame, we further added a frame-level OCR task.",
                "In addition to OCR and video captioning tasks, we have designed a series of reasoning-enhanced tasks to help the model better understand contextual relationships in short videos. These include:",
                " Frame-level re-ordering: Given a set of shuffled video frames, our model is required to predict their original chronological order, which enhances its ability to grasp temporal progression and logical flow.",
                " Multiple video matching: Provided with a group of related videos and a set of candidate videos, our model is required to identify the most contextually relevant candidate, which refines its understanding of semantic connections across different videos.",
                "8"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "Data processing",
                "Image and text interleaving",
                "Video data processing"
            ],
            "page_summary": "The data processing techniques discussed include garbled character recognition, low-resolution/broken image filtering, and text-image similarity validation. Additionally, there's a focus on interleaving image and text data, and a detailed explanation of video data processing, including interleaved video-ASR, video recaption, frame-level OCR annotation, and reasoning-enhanced tasks.",
            "page_number": "8"
        }
    },
    "9": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {
                "9": {
                    "28": {
                        "image_path": "/content/llm_paper_images/figure-9-28.jpg",
                        "image_heading": "a snowflorer logo with a white background"
                    },
                    "25": {
                        "image_path": "/content/llm_paper_images/figure-9-25.jpg",
                        "image_heading": "a small orange fire with a white background"
                    },
                    "30": {
                        "image_path": "/content/llm_paper_images/figure-9-30.jpg",
                        "image_heading": "a small orange leaf on a white background"
                    },
                    "24": {
                        "image_path": "/content/llm_paper_images/figure-9-24.jpg",
                        "image_heading": "a black and white image of a document"
                    },
                    "21": {
                        "image_path": "/content/llm_paper_images/figure-9-21.jpg",
                        "image_heading": "a small orange leaf on a white background"
                    },
                    "23": {
                        "image_path": "/content/llm_paper_images/figure-9-23.jpg",
                        "image_heading": "a small orange fire with a white background"
                    },
                    "31": {
                        "image_path": "/content/llm_paper_images/figure-9-31.jpg",
                        "image_heading": "a document with a line of text"
                    },
                    "27": {
                        "image_path": "/content/llm_paper_images/figure-9-27.jpg",
                        "image_heading": "a small orange leaf on a white background"
                    },
                    "26": {
                        "image_path": "/content/llm_paper_images/figure-9-26.jpg",
                        "image_heading": "a black and white image of a document"
                    },
                    "22": {
                        "image_path": "/content/llm_paper_images/figure-9-22.jpg",
                        "image_heading": "a small orange leaf on a white background"
                    },
                    "29": {
                        "image_path": "/content/llm_paper_images/figure-9-29.jpg",
                        "image_heading": "a small orange fire with a white background"
                    },
                    "20": {
                        "image_path": "/content/llm_paper_images/figure-9-20.jpg",
                        "image_heading": "a small orange leaf on a white background"
                    },
                    "19": {
                        "image_path": "/content/llm_paper_images/figure-9-19.jpg",
                        "image_heading": "a snowflorer logo with a white background"
                    }
                }
            },
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {
            "SigLIP Pre-training": [
                "",
                "",
                "t",
                "",
                "",
                "",
                "",
                "",
                ":",
                ""
            ],
            "Native-Resolution": [
                "(2D RoPE)",
                "",
                "",
                "",
                "Stage 0 Image-Text Matching",
                "Stage 1 ViT-LLM Alignment",
                "Stage2 Stage3 Multi-task Pre-training Annealing (with mode! merging)"
            ],
            "3.2 Training Recipe": [
                "We employ a four-stage progressive training strategy to build a powerful multi-modal foundation model with strong vision-language alignment capabilities. The training pipeline, illustrated in Figure 3, is meticulously designed to ensure that each stage has a clear and interconnected objective.",
                "The Vision Transformer (Dosovitskiy et al. (2020)) (ViT) is initialized with weights from the siglip-so400m- patch14-384 model and undergoes continuous pre-training using the SigLIP (Zhai et al. (2023)) contrastive loss function. This stage focuses on adapting the vision encoder to our internal data distribution. We incorporate native dynamic resolution processing (akin to NaViT (Dehghani et al. (2023))), which preserves the original aspect ratio of images to the greatest extent possible. Additionally, 2D Rotary Position Embeddings (Su et al. (2024)) (RoPE) are integrated to enhance the models extrapolation capabilities when processing images of varying resolutions.",
                "Stage 1: cross-modal alignment: The language model is initialized from Qwen3-8B (Yang et al. (2025)). During this stage, the parameters of both the vision and language models are frozen. Training is focused on optimizing the projection MLP layer. With large-scale datasets, we establish a robust alignment between cross-modal features, laying the groundwork for the subsequent learning phase.",
                "Stage 2: multi-task pre-training: All model parameters are unfrozen for end-to-end optimization using a diverse set of multi-task training data. The data in this stage encompasses a wide range of common vision-language tasks, including Image Captioning, Optical Character Recognition (OCR), Grounding, Visual Question Answering (VQA), and interleaved image-text data. This process significantly enhances the models fundamental visual understanding capabilities.",
                "Stage 3: annealing: This stage involves an annealing phase where the model is fine-tuned on a curated set of high-quality data. The primary goal is to address the issue of insufficient exposure to high-quality samples during the large-scale, broader training of Stage 2. Through optimized learning strategies and data mixtures, we further refine the models nuanced understanding and capabilities.",
                "Model merging: The performance of pre-trained models on downstream tasks is highly sensitive to the training data mixture, an effect that is particularly pronounced in smaller models (Li et al. (2025b)). Relying on a fixed data ratio selected based on a validation set can amplify the models intrinsic biases, leading to discrepancies between benchmark performance and real-world application. To alleviate this, in the final phase of pre-training, we explore a homogeneous-heterogeneous merging technique. This involves averaging the weights of models that have been annealed with different data mixtures. This approach preserves the diverse capabilities of the individual models while reducing overall bias and enhancing model robustness."
            ],
            "4 Post-Training": [
                "As shown in Figure 4 and Figure 5, the post-training process for Kwai Keye-VL is a meticulously designed, two-stage methodology engineered to cultivate a comprehensive suite of capabilities. The initial phase, encompassing the first two steps, is dedicated to establishing foundational performance in natural image understanding and text interaction. The subsequent stage, comprising the final three steps, focuses on progressively enhancing the models sophisticated reasoning abilities.",
                "9",
                ""
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "Machine Learning",
                "Pre-training",
                "Vision Transformer",
                "Language Model",
                "Multi-task Pre-training",
                "Model Merging",
                "Post-Training"
            ],
            "page_summary": "This page discusses the four-stage progressive training strategy for building a powerful multi-modal foundation model with strong vision-language alignment capabilities. It includes SigLIP pre-training, native-resolution processing, cross-modal alignment, multi-task pre-training, annealing, and model merging. The post-training process is also outlined, focusing on natural image understanding, text interaction, and sophisticated reasoning abilities.",
            "page_number": "9"
        }
    },
    "10": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {
                "10": {
                    "35": {
                        "image_path": "/content/llm_paper_images/figure-10-35.jpg",
                        "image_heading": "a diagram of the two different layers"
                    },
                    "36": {
                        "image_path": "/content/llm_paper_images/figure-10-36.jpg",
                        "image_heading": "a black and white image of a brain"
                    },
                    "32": {
                        "image_path": "/content/llm_paper_images/figure-10-32.jpg",
                        "image_heading": "a graph graph with a line of numbers"
                    },
                    "37": {
                        "image_path": "/content/llm_paper_images/figure-10-37.jpg",
                        "image_heading": "the logo for the international association of the united states"
                    },
                    "33": {
                        "image_path": "/content/llm_paper_images/figure-10-33.jpg",
                        "image_heading": "a molecule molecule with a white background"
                    },
                    "34": {
                        "image_path": "/content/llm_paper_images/figure-10-34.jpg",
                        "image_heading": "a black and white image of a molecule"
                    }
                }
            },
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {
            "Base Model": [
                "Step 1.1 Step 1.2 Mixed ep! Supervised ~P  Preference Fine-Tuning cee Optimization",
                "400k Open. source data"
            ],
            "ial niiakakad": [],
            "ROK RET data": [
                "30k Human annotated data",
                "70k task galaxy 200k filtered QA pair Human annotated image/video captions",
                "Figure 4: No-Reasoning Training Pipeline: The process begins with a Base model, proceeds through Supervised Fine-Tuning (utilizing 70k task galaxy, 200k filtered QA pairs, and human-annotated im- age/video captions), and culminates in Mixed Preference Optimization with various preference data sources (400k open-source data, 10k RFT data, 90k text data, and 30k human-annotated data).",
                "| Il. Reasoning Training",
                "Step Il.1 CoT Cold Start",
                "Step II.3 Iterative Alignment",
                "",
                "@"
            ],
            "Select high-frequency think path": [
                "LLM",
                "Step II.2 Mix -Mode RL"
            ],
            "Mixed Preference Optimization": [],
            "RL Model": [
                "",
                "Modes 7 | Thinking Mode Auto-Thinking I | Mode Non-Thinking L Mode | Nee ee ee",
                "Rewards 1. Outcome Reward",
                ". 2. Consistency Reward Y"
            ],
            "Agentic Mode": [
                "|",
                "|",
                "1. Repetition score 2. Instruction following score logical score",
                "3."
            ],
            "es ee": [
                "Figure 5: Reasoning Training Pipeline: The process consists of three key steps: CoT Cold Start (involving sampling, quality checks, and high-frequency think path selection from a data pool to create a CoT Cold- Start Dataset), Mix-Mode RL (featuring Thinking Mode, Non-Thinking Mode, Auto-Thinking Mode, and Agentic Mode with Outcome and Consistency Rewards), and Iterative Alignment (implementing an RL Model with Mixed Preference Optimization, Rejection Sampling, and Preference Data Filtering based on repetition, instruction following quality, and logical scores).",
                "The training begins with large-scale Supervised Fine-Tuning (SFT) to elevate performance across a wide array of tasks. This is followed by Mixed Preference Optimization (Wang et al. (2024b)) (MPO) to solidify model stability and efficacy in non-reasoning contexts. The third and fourth stages mark a significant leap in cognitive function, introducing Chain-of-Thought (Wei et al. (2022)) (CoT) capabilities through Cold-Start and further refining them via RL. The final stage employs iterative alignment to construct high- quality preference data, which empowers the model to autonomously select the appropriate reasoning mode, thereby ensuring robust and stable performance in practical applications.",
                "To prevent data leakage during post-training, we perform strict data deduplication by removing training samples that are highly similar to common benchmark examples, thereby ensuring fair and unbiased evaluation. Detailed information can be found in Appendix A.",
                "4.1 No-Reasoning Training: Establishing Foundational Performance",
                "This initial phase establishes the models core performance and stability in non-reasoning scenarios through two sequential steps (Figure 4).",
                "10"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "Machine Learning",
                "Model Training",
                "Reinforcement Learning",
                "Data Preprocessing",
                "Performance Optimization",
                "Artificial Intelligence"
            ],
            "page_summary": "This page discusses the training process of a machine learning model, including large-scale supervised fine-tuning, mixed preference optimization, and chain-of-thought capabilities. It also mentions reinforcement learning modes and rewards, data preprocessing steps, and performance optimization techniques. The page includes figures and a table, and is part of a larger discussion on a model's development.",
            "page_number": "10"
        }
    },
    "11": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {},
        "page_metadata": {
            "topics_detected": [],
            "page_summary": "",
            "page_number": "11"
        }
    },
    "12": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {},
        "page_metadata": {
            "topics_detected": [],
            "page_summary": "",
            "page_number": "12"
        }
    },
    "13": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {
            "5 Training Infrastructure": [
                "To ensure efficient and stable training of the billion-parameter model, we implement deep optimizations across three key areas: parallelization strategy, load balancing, and fault tolerance.",
                "Optimized hybrid parallelism: We adopt a hybrid parallelization strategy combining Data Parallelism (DP) and Sequence Parallelism (SP) to scale efficiently across our large compute cluster. Our DP im- plementation is deeply integrated with the ZeRO (Rajbhandari et al. (2020)) optimizer. This not only reduces per-device memory pressure by sharding optimizer states, gradients, and parameters, but more critically, it enables effective computation-communication overlap. During backpropagation, gradient calculation can proceed in parallel with the gradient synchronization communication, effectively hiding communication latency and improving overall training throughput.",
                "Dynamic load balancing: To address the severe computational load imbalance caused by variable input sizes (images/videos) in multimodal training, we implement a global greedy balancing strategy. At each global step, this strategy evaluates the FLOPs of each sample in the global batch, sorts all samples in descending order by their FLOPs, and then greedily reassigns them to the parallel group with the current lowest computational load. This mechanism dynamically flattens the load across all nodes, minimizing hardware idle time and significantly boosting overall training speed.",
                "13"
            ]
        },
        "page_metadata": {
            "topics_detected": [],
            "page_summary": "",
            "page_number": "13"
        }
    },
    "14": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {
            "14": {
                "4": {
                    "table_path": "/content/llm_paper_images/table-14-4.jpg",
                    "table_heading": "",
                    "table_data": " \n\nModels ImageNet-1K ImageNet-V2 ImageNet-A ImageNet-R ImageNet-S ObjectNet\n\nBase (SigLIP-400M-384-14) 83.08 77.34 82.22 95.78 74.59 76.99\n+ 1D interpolation 82.02 75.96 80.92 94.50 70.74 67.58\n+ 1D interpolation + 2D RoPE 82.65 76.80 83.26 95.22 72.59 78.70\n\f"
                }
            }
        },
        "formula": [],
        "footer": [],
        "topic_content": {
            "6 Evaluation": [],
            "6.1 Zero-shot Image Classification of ViT": [
                "To validate that our continue trained native-resolution ViT is able to capture promising visual representa- tions, we conduct a wide-used zero-shot image classification benchmark analysis. In our evaluation, we perform a comparative analysis between the base SigLIP model and its two native-resolution position embedding variants, leveraging the CLIP Benchmark2 framework with text prompt template3.",
                "The evaluation covers six benchmark datasets: ImageNet-1K, ImageNet-V2, ImageNet-A, ImageNet- R, ImageNet-S and ObjectNet, and its results are shown in Table 2. From it, we have the following observations: (1) Compared with base SigLIP model, our 1D interpolation position embedding native- resolution model variant has slightly performance degeneration, the reason might be the interpolated 1D position encoding cannot uniquely identify the underlying 2D patch arrangement. For instance, a sequence of 196 patches may correspond to multiple distinct spatial configurations (e.g., 1414, 728, or 287), leading to ambiguous spatial localization during feature projection. (2) With 2D RoPE modification, our ViT could clearly perceive the shape of the image, and showing competitive results with Base SigLIP performance (the best and runner-up results). We think the reason maybe our continued pretraining corpus sharing the same distribution with our MLLMs, rather than the Image-Text matching task."
            ],
            "6.2 Public Benchmarks": [
                "In this section, we evaluate Keye-VL across various benchmarks. For general vision-language tasks, we select MMMU (Yue et al. (2024)), AI2D (Kembhavi et al. (2016)), V* (Wu &Xie (2024)), BLINK (Fu et al. (2024)), VLMS are Blind (Rahmanzadehgervi et al. (2024)), ZeroBench (Roberts et al. (2025)), VisuLogic (Xu et al. (2025b)), RealWorldQA (X (2025)), SimpleVQA (Cheng et al. (2025)), MMStar (Chen et al. (2024a)), MMVP (Tong et al. (2024)), HallusionBench (Guan et al. (2024)) and All-Angles-Bench (Yeh et al. (2025)). For Doc and OCR tasks, we select ChartQA (Masry et al. (2022)), CharXivDQ (Wang et al. (2024d)), and OCRBench (Liu et al. (2024d)). For MATH tasks, we select MathVision (Wang et al. (2024a)), MathVistaMINI (Lu et al. (2023)), MathVersevision (Zhang et al. (2024a)), OlympiadBench (He et al. (2024)), WeMath (Qiao et al. (2024)), LogicVista (Xiao et al. (2024)), and DynaMath (Zou et al. (2024)). For public Video tasks, we select Video-MME(Fu et al. (2025a)), Video-MMMU (Hu et al. (2025b)), TempCompass (Liu et al. (2024c)), LongVideoBench (Wu et al. (2024a)), and MMVU (Zhao et al. (2025)).",
                "We compare the performance of Keye-VL in Thinking and Auto-Think mode with state-of-the-art models of a similar scale, including Qwen2.5-VL 7B, InternVL3-8B (Zhu et al. (2025)), MiMo-VL-7B-RL (Xiaomi (2025)), and proprietary models such as GPT-4o and Claude-3.7-Sonnet.",
                "2https://github.com/LAION-AI/CLIP_benchmark",
                "3https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Prompt_Engineering_ for_ImageNet.ipynb#scrollTo=sRqDoz1Gbsii",
                "14"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "Zero-shot Image Classification",
                "Benchmark Analysis",
                "Position Embedding Variants",
                "CLIP Benchmark",
                "Table 2",
                "ViT Model",
                "ImageNet Datasets",
                "Performance Evaluation"
            ],
            "page_summary": "This page conducts a zero-shot image classification benchmark analysis on a native-resolution ViT model and its two position embedding variants, comparing them with the base SigLIP model. The evaluation is performed using the CLIP Benchmark framework and covers six benchmark datasets. The results show that the 2D RoPE modification variant has competitive performance with the Base SigLIP model.",
            "page_number": "14"
        }
    },
    "15": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {
            "15": {
                "5": {
                    "table_path": "/content/llm_paper_images/table-15-5.jpg",
                    "table_heading": "",
                    "table_data": " \n\n \n\n \n\n \n\n \n\n \n\n \n\nBaiichinark Keye-Vi Keye-VL Qwen2.5-VL InternVL3  MiMo-VL GPT-4o0 Claude 3.7\n8B-Thinking  8B-Auto-Think 7B 8B 7B-RL Sonnet\nGeneral\nMMMU,al 71.4 66.8 58.6 62.7 66.7 70.7 69.8\nAI2D 86.7 85.8 83.9 85.2 83.5 82.6 81.4\nve 69.6 67.9 79.1 71.2 81.7 73.9 *\nBLINK, 52.0 52.5 56.4 55.5 62.4 60.0 62.3\nVLMs are Blind 57.1 61.0 37.4 36.8 79.4 49.8 72e1,\nZeroBenchgp, 15.2 11 0.0 0.0 - - -\nVisuLogic 25.6 21.1 20.0 26.1 - - -\nRealWorldQA 67.7 66.1 68.2 70.6 : : -\nSimpleVQA 41.6 36.9 41.4 35.1 - - =\nMMStar 75.5 72.8 64.9 68.4 70.8 - *\nMMVP 79.0 80.3 78.0 78.3 : : -\nHallusionBench 67.0 572 55.7 49.4 61.9 - -\nAll-Angles-Bench 47.3 50.3 49.4 50.7 - -\nDoc & OCR\nChartQA 86.3 72.5 90.2 89.6 91.7 86.7 92.2\nCharXivpg 77.7 75.2 73.9 73.6 86.8 86.5 89.5\nOCRBench 85.1 85.3 89.7 88.0 86.6 84.3 80.6\nMATH\nMathVision 46.0 424 26.2 28.8 60.4 31.2 -\nMathVistaygnt 80.7 75.2 66.8 70.7 81.5 63.8 *\nMathVersevision 59.8 40.8 41.2 32.4 71.5 49.9 \u201c\nOlympiadBench 54.8 45.2 19.4 25.9 59.4 259 -\nWeMath 60.7 58.6 37.7 38.5 66.3 50.6 =\nLogicVista 54.8 50.6 44.5 43.6 61.4 54.4 =\nDynaMath 37.3 35.3 20.1 23.9 45.9 54.4 -\nVideo\nVideo-MME w/o sub, 67.7 59.7 65.1 66.3 67.4 71.9 -\nVideo-MMMU 57.6 56.9 474 48.9 43.3 = -\nTempCompass 71.5 58.2 68.3 70.8 68.3 - -\nLongVideoBench 62.8 64.8 59.3 63.9 50.0 7 =\nMMVU 66.1 60.3 45.5 39.4 58.2 : -\nShort-Video\nCPV 55.1 55.9 20.1 15.0 16.7 : -\nHot Videos 4.3 55.0 46.4 52.3 49.0 - 7\nAggregation \u2014\u2014\nCollection Order 84.4 82.0 59.8 64.8 78.7 - -\nForographic 72.0 70.4 56.1 57.1 68.3 3 :\nHigh Like 55.3 53.4 47.9 47.0 51.1 = -\nSPU 87.1 849 813 75.6 819 a a\n\n \n\n \n\n \n\f"
                }
            }
        },
        "formula": [],
        "footer": [],
        "topic_content": {},
        "page_metadata": {
            "topics_detected": [
                "Artificial Intelligence",
                "Natural Language Processing",
                "Machine Learning",
                "Table Data",
                "Model Comparison"
            ],
            "page_summary": "This page compares various models in artificial intelligence and natural language processing, including General, Doc & OCR, MATH, Video, and Short-Video models. The table data provides a performance overview of each model across different categories.",
            "page_number": "15"
        }
    },
    "16": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {
            "6.3 Internal Benchmarks": [
                "Despite extensive evaluations on a wide array of public benchmarks, these benchmarks exhibit numerous limitations that necessitate a focused effort on developing a proprietary, internal evaluation suite. The primary issues are as follows:",
                " Benchmark contamination: An unavoidable limitation of public datasets is the potential for their data to have been inadvertently or deliberately exposed during the models training process. This phenomenon, known as benchmark contamination, can lead to exaggerated performance metrics, reduc- ing the sensitivity of public benchmarks to subtle model improvements and thus failing to reflect the models capabilities. To obtain a reliable assessment of a models genuine performance, it is imperative to construct an internal evaluation benchmark that is insulated from training data contamination.",
                " Limited language coverage: The majority of public benchmarks are predominantly focused on English- language scenarios. This significantly constrains the exploration of a models abilities within the context of native Chinese applications. English benchmarks are incapable of adequately covering or measuring the unique expressions, profound cultural contexts, and diverse local needs inherent to the Chinese language environment. Therefore, building a benchmark that can effectively evaluate a models Chinese language capabilities is of paramount importance.",
                " Insufficient task and domain coverage: Existing general-purpose evaluation benchmarks primarily concentrate on fundamental perception and simple reasoning abilities. They fail to comprehensively cover a multi-dimensional spectrum of capabilities, including fine-grained perception, cross-modal ground- ing, language modeling, complex reasoning, and safety & robustness. Furthermore, current benchmarks lack a focus on real-world application scenarios, particularly core business needs such as multimodal understanding in short video communities. This deficiency results in an evaluation framework that cannot effectively reflect a models performance in practical business tasks, severely hindering the assessment of its practical value and potential for deployment.",
                " Monotonous task difficulty and evaluation format: The task types and question formats (e.g., true/- false, multiple-choice, fixed-answer questions) in existing benchmarks are relatively simple and uniform. They are insufficient for comprehensively measuring a models capacity to handle complex, open-ended question answering, which more closely mirrors authentic user interactions. Constructing a more challenging evaluation benchmark that supports open-ended generative responses is better suited to simulate real-world user and business interactions, thereby enabling a more accurate assessment of the models generalization capabilities in practical scenarios.",
                "6.3.1 Design Strategies and Core Principles",
                "In response to the limitations inherent in existing public evaluation benchmarks and motivated by the aforementioned needs, we construct an internal evaluation benchmark set, and the full breakdown for the benchmark is detailed in Table 10. This structure is intended to fully reflect the models performance and application potential across different sub-scenarios. While ensuring robust coverage of textual abilities, we place a special emphasis on multimodal capabilities, particularly for images and videos. In developing the evaluation benchmarks for image and video modalities, we adhere to the following core principles:",
                " Targeted capabilities and business relevance: The internal benchmark is oriented towards measuring the foundational models ability to handle real user interaction tasks within a native Chinese context. We particularly focus on the models performance in open-ended question answering scenarios that",
                "4The detailed construction process of KC-MMbench is shown in Appendix B.",
                "16"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "Artificial Intelligence",
                "Benchmarks",
                "Model Evaluation",
                "Chinese Language Processing",
                "Multimodal Understanding",
                "Performance Metrics"
            ],
            "page_summary": "This text discusses the limitations of public benchmarks for evaluating AI models and proposes the development of proprietary, internal evaluation suites. It emphasizes the need for benchmarks that are insulated from training data contamination, cover multiple languages and tasks, and simulate real-world user interactions. The text also introduces a strategy for designing an internal evaluation benchmark, focusing on real user interaction tasks within a native Chinese context and multimodal capabilities for images and videos.",
            "page_number": "16"
        }
    },
    "17": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {
                "17": {
                    "42": {
                        "image_path": "/content/llm_paper_images/figure-17-42.jpg",
                        "image_heading": "a person holding a pen and a book"
                    },
                    "43": {
                        "image_path": "/content/llm_paper_images/figure-17-43.jpg",
                        "image_heading": "a col of nine pictures of a woman ' s jacket"
                    },
                    "41": {
                        "image_path": "/content/llm_paper_images/figure-17-41.jpg",
                        "image_heading": "a woman sitting on the floor"
                    },
                    "40": {
                        "image_path": "/content/llm_paper_images/figure-17-40.jpg",
                        "image_heading": "the basketball players are playing in the court"
                    },
                    "39": {
                        "image_path": "/content/llm_paper_images/figure-17-39.jpg",
                        "image_heading": "a group of people standing around a building"
                    },
                    "38": {
                        "image_path": "/content/llm_paper_images/figure-17-38.jpg",
                        "image_heading": "a woman in a red dress is standing on a stage"
                    }
                }
            },
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {
            "Task Definition": [
                "Pornographic The task of justifying, whether short video comments contain Comment oe araphie P fh Pt content.",
                "Image/ Video",
                "",
                "Input Please answer the question based on the video cover and extracted video information. The video information is as follows: Video Title: The wind from south of the Yangtze River blows on the birth of all things. Video ASR: You say those sleepless nights and white dreams are hazy and hypoxic... User Comment: Every time I watch her dance, I get a special feeling, so good-looking, a queenly vibe [Lets party together]. Please determine whether the comment is pornographic based on the information above. The final output should be \"Yes\" or \"No.",
                "No"
            ],
            "Output": [],
            "Conlection": [
                "The task of determining the logical order between multiple videos with the same topic.",
                "",
                "The following is a list of information for 3 videos. It is known that the 3 videos are from the same video collection. Please sort them based on the video content. Please output ieee Video 2, your answer in the format of a list, with the video numbers in sequential order, such as: [Video 1, Video 3, Video 2]."
            ],
            "The task of": [
                "determining whether Hot Videos mning  multiple videos Aggregation holon to the come toe : P",
                "",
                "The following is a list of information for 3 videos. Please determine if the videos in the given list belong to the same topic as the first video. Each image corresponds to a screenshot of a different video. Please output your answer  : ao : in the format of a list. Each item in the list represents the judgment of whether the corresponding video belongs to : . ng vic the same topic as the first video. The first video also needs to be included in the list. Example: Assuming there are 3 videos in the list, the output should be \"Yes Yes No\", which means the second video belongs to the same topic as the first video, and the third video does not belong to the same topic as the first video."
            ],
            "Yes Yes No": [
                "High Like 8",
                "A binary classification task to determine the rate of likes of a short video.",
                "",
                "Please answer the question based on the video cover and extracted video information. This is a video on a short video platform. Please judge whether the video can get ahigh like rate on the platform based on the above video content information. The final output needs to be \"Yes\" or \"No\". If it can get a high like rate, output \"Yes\", otherwise output \"No\".",
                ",.. .",
                "The task of SPU (Standard determining whether 3 two items are the Product Unit) same product in e-commerce.",
                "",
                "Please determine whether the two given images belong to the same product.",
                "Yes",
                "CPV (Category Te tsk of predicting Property Value) Productattributes in e-commerce.",
                "",
                "You are an e-commerce Al assistant equipped with a range of capabilities to support various e-commerce operations. There is now an attribute prediction task. You need to , , . : . (Womens clothing summarize the attributes of the product from the specified . ummar \\ for middle-aged and dimensions based on the main product image and a , : ea elderly: Jacket/coat, information (product title and product description)  : 1 ne Pr . Sleeve type: provided to you. The following is the relevant information -Ree var \"Whether of the product whose attributes need to be predicted: thene ica fur collar: Product Title: [Return shipping compensation] 25015 'No, Placket womens new style fashionable jacket [small grid] BY Product Description: None. Please summarize the attribute information of this product from the dimensions of Womens clothing for middle-aged and elderly: ['Small camisole/small sling, Jacket/coat...], Sleeve type: [Regular, Petal sleeve...], Whether there is a fur collar: .., 'Placket'..., answer in Chinese, and the final attribute 'Single-breaste Sleeve length: Long sleeve, Clothing length: Regular style}",
                "summary should be output in json string format.",
                "Table 4: Kuaishou Community Multimodal Benchmark: KC-MMBench is a self-constructed short-video benchmark for MLLMs, which contains 6 task categories and 1840 instances.",
                "require generating complete and valuable responses, in order to assess its generalization capabilities in real business contexts more accurately. In detail, the benchmark not only covers general-purpose capabilities (such as perception and reasoning) but also evaluates potential performance in business scenarios rarely addressed by current open-source benchmarks, such as multimodal understanding in short video communities. This design aims to comprehensively assess the models applied value and ensure it can effectively support actual business requirements.",
                " Comprehensive and fine-grained capability taxonomy: We design a multi-level capability taxonomy to facilitate the construction of diverse and fine-grained data tasks and to support the evaluation of the models capabilities at various levels. At the macro-capability level (Layer 2), the evaluation framework covers a range of fundamental abilities, including basic visual skills, multimodal understanding tasks, and complex reasoning, while also incorporating safety and robustness. At the specific capability level (Layer 3/4), we further refine these dimensions into a comprehensive set of tasks.",
                "17"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "task definition",
                "image and video analysis",
                "e-commerce product analysis",
                "benchmarking and evaluation"
            ],
            "page_summary": "This page discusses various tasks related to image and video analysis, such as determining pornographic content in comments and logical order between videos. It also touches upon e-commerce product analysis tasks like determining if two images belong to the same product and predicting product attributes. The page further introduces a benchmark for evaluating models on multimodal understanding tasks in short video communities.",
            "page_number": "17"
        }
    },
    "18": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {
            "18": {
                "7": {
                    "table_path": "/content/llm_paper_images/table-18-7.jpg",
                    "table_heading": "",
                    "table_data": " \n\nubset Capability (Level 2) Keye-VL-8B Quwen2.5-VL-7B InternVL3-8B MiMo-VL-7E\n\n \n\n \n\nthinking non-thinking non-thinking thinking\nOverall (Average) 3.33, 3.31 2.72 2.75\nVisual Element Recognition 3.89 3.54 3.40 3.46\nTemporal Information Understanding 2.92 2.96 2.46 2.25\nideo Description Ability 3.08 3.58 2.92 2.58\nbset Creative Ability 3.17 3.50 1.83 2.67\nuPset Knowledge-based QA 2.78 2.00 241 217\nReasoning Ability 3.31 3.50 2.85 2.88\nDomain-specific Expertise 3.36 4.40 3.30 3.09\nRobustness 3.50 3.42 2.08 2.17\nOverall (Average) 3.81 3.69 3.67 3.71\nVisual Recognition 3.97 3.91 3.96 3.98\nnage Visual Understanding 3.70 3.27. 3.37 3.73\nubset Basic Description 4.00 3.91 3.68 3.63\nVisual Storytelling 3.63 3.94 3.75 3.31\n\nMulti-image Analysis 3.15 277 2.69 2.92\n\n \n\f"
                },
                "6": {
                    "table_path": "/content/llm_paper_images/table-18-6.jpg",
                    "table_heading": "",
                    "table_data": "Subset Capability (Level 2) Keye-VL-8B | Qwen2.5-VL-7B InternVL3-8B MiMo-VL-7B\n\n \n\n \n\nthinking non-thinking non-thinking thinking\n\nOverall (Average) 3.33 3.31 272 2.75\nCorrectness 3.34 3.41 2.87 3.07\n\nVideo Comprehensiveness 4.36 3.93 3.32 3.43\nSubset Relevance 4.83 4.85 4.59 4.43\nFluency 4.89 4.89 4.83 4.79\nCreativity 3.75 3.50 2.75 2.92\nOverall (Average) 3.81 3.69 3.67 3.71\nCorrectness 4.05 3.82 3.91 3.87\n\nImage Comprehensiveness 4.49 4.43 4.40 4.49\nSubset Relevance 4.91 4.87 4.93 4.74\nFluency 4.89 4.94 4.87 4.72\n\nCreativity 3.69 4.06 3.69 3.81\n\n \n\f"
                }
            }
        },
        "formula": [],
        "footer": [],
        "topic_content": {
            "Video": [],
            "Subset": [
                "Table 6: Detailed capability comparison of Keye-VL with other models on the internal benchmark: The evaluation examines specific capabilities ranging from basic recognition to complex reasoning and multi-image analysis. The highest scores are marked in bold and the second highest are underlined.",
                " Authentic and diverse data coverage: To ensure the generalizability of our evaluation and mitigate the risk of overfitting, our benchmark emphasizes diversity in both videos and questions. Our data sources are authentic and reliable; the general benchmark utilizes a large volume of diverse and timely real-world image and video data. The scope of our sampled data is extensive, including various image categories (natural, text-based, artificial) and short videos with diverse frame rates, resolutions, and subjects (people, landscapes, objects, IPs), as well as complex cinematographic and motion elements.",
                " Mitigation of benchmark contamination: To build an evaluation benchmark that genuinely reflects model capabilities and is free from contamination, we have adopted several maintenance strategies. We employ methods such as timestamp verification to prevent data leakage and conduct manual, item-by-item quality checks to ensure data accuracy. Furthermore, we establish a dynamic maintenance mechanism featuring regular updates and on-demand supplementation. This involves continuously refreshing redundant data while adding new tasks that target model weaknesses, ensuring the bench- mark remains current and capable of capturing the latest model advancements.",
                " Hierarchical evaluation methodology: We employ a multi-level evaluation system to comprehensively assess the models open-ended question answering capabilities. This system first involves a Five- Dimensional Scoring of model responses based on metrics of correctness, relevance, comprehensiveness, fluency, and creativity. To facilitate a holistic comparison, we then calculate a Composite Score by applying task-specific weights to these dimensions.",
                "18"
            ],
            "Image": []
        },
        "page_metadata": {
            "topics_detected": [
                "Artificial Intelligence",
                "Model Comparison",
                "Table Data",
                "Benchmarking",
                "Evaluation Metrics",
                "Video and Image Processing"
            ],
            "page_summary": "This page presents a detailed comparison of Keye-VL model with other models in terms of various capabilities like basic recognition, complex reasoning, and multi-image analysis. The evaluation is done on an internal benchmark using authentic and diverse data sources. A hierarchical evaluation methodology is adopted involving five-dimensional scoring and composite score calculation. The page also highlights the importance of maintaining a clean and updated benchmark.",
            "page_number": "18"
        }
    },
    "19": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {},
        "page_metadata": {
            "topics_detected": [],
            "page_summary": "",
            "page_number": "19"
        }
    },
    "20": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {},
        "page_metadata": {
            "topics_detected": [],
            "page_summary": "",
            "page_number": "20"
        }
    },
    "21": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {
            "21": {
                "8": {
                    "table_path": "/content/llm_paper_images/table-21-8.jpg",
                    "table_heading": "",
                    "table_data": "Benchmark MathVistayin1 MMStar HallusionBench OCRBench\nThinking Ratio 0.35 0.34 0.08 0.00\n\n \n\f"
                }
            }
        },
        "formula": [],
        "footer": [],
        "topic_content": {
            "6.4 Quantitative Results": [
                "We provide a selection of qualitative examples illustrating the capabilities of the Kwai Keye-VL model in various aspects. Our presentation is divided into two dimensions: Modality and Thinking Mode. In Appendix C.1, we demonstrate the Kwai Keye-VL models capabilities in pure text (Appendix C.1.1), images (Appendix C.1.2), and videos (Appendix C.1.3), including public benchmark cases and business scenario cases. In Appendix C.2, we showcase the two thinking modes of the Kwai Keye-VL model: Agentic Thinking (Appendix C.2.1) and Auto Thinking (Appendix C.2.2)."
            ],
            "7 Discussion": [],
            "7.1 Mutual Enhancement between Reasoning and Non-Reasoning Data": [
                "As discussed in Section 4.2.1, following the non-reasoning stage, we introduce the CoT-Mix dataset to cold-start Keye-VLs reasoning capabilities. During this phase, Keye-VL acquires proficiency in both the Non-Thinking and Thinking modes. We evaluate the impact of Step II.1 (cold start with CoT-Mix data) by comparing Keye-VLs performance before and after this training stage. In the Non-Thinking and Thinking modes, Keye-VLs performance improves by 5.67% and 8.22% on MMMU, and by 2.95% and 7.97% on HallucinationBench, respectively. These results demonstrate that training on CoT-Mix data strengthens both perceptual and reasoning abilities. Furthermore, since similar non-reasoning data is also employed during Steps I.1 and I.2, the observed performance gains in the Non-Thinking mode are primarily attributable to the inclusion of long CoT reasoning data. This suggests that the integration of long CoT data also enhances the models capability in the Non-Thinking mode, which aligns with recent findings reported in ERNIE 4.5 ( Team (2025b)).",
                "On MathVista, Keye-VLs accuracy increases by -1.1% (Non-Thinking) and 7.9% (Thinking), suggesting that the model spontaneously establishes the connection between the Thinking mode and complex logical reasoning. Based on this observation, we introduce Auto-Think mode in step II.2 to further enhance Keye-VLs ability to select reasoning modes based on difficulty and improve overall performance."
            ],
            "7.2 Performance Gain from RL Training": [
                "How to synchronously and stably improve the models performance across various tasks is one of the core challenges of RL. In Step II.2 (Section 4.2.2), we attempt to achieve this by adopting the Mix-Mode strategy. In our conception, Non-Thinking and Thinking Modes are respectively suitable for simple and complex tasks. Therefore, during the RL process, we apply Keye-VL to generate samples using a mix of both two modes to comprehensively enhance its performance across various tasks, and we incorporate the Auto-Think mode as we discussed before.",
                "The evaluation results show that after RL training, Keye-VL achieves an average improvement of 1.44%/2.17% in Non-Thinking/Thinking mode across 10 benchmarks. Its performance increase by 0%/0.93% to 4.18%/5.73% on 9 benchmarks, with a decrease of only 1.11%/1.2% on MMMU/OCRBench. This indicates that mix-mode RL strategy achieves a comprehensive and synchronous improvement."
            ],
            "7.3 Analysis about Auto-Think Mode": [
                "Benchmark MathVistaMINI MMStar HallusionBench OCRBench Thinking Ratio 0.35 0.34 0.08 0.00",
                "Table 7: Proportions of Keye-VL selecting the Thinking mode rather than the Non-Thinking mode across various benchmarks when operating in Auto-Think mode.",
                "To evaluate Keye-VLs ability to spontaneously select the reasoning mode by task type and difficulty, we calculate the proportions of Keye-VL choosing the Thinking mode during Auto-Think inference across four different benchmarks (Table 7). It turns out that on the more challenging and logic-reasoning-oriented MAthVista and MMStar, Keye more frequently selects the Thinking mode. However, Non-Thinking remains the predominant mode, as the difficulty of most samples is limited. On the HallusionBench, Keye-VL selects the Thinking mode only in few instances. In these cases, Keye-VL engages in reflection to",
                "21"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "Artificial Intelligence",
                "Natural Language Processing",
                "Keye-VL model",
                "Reasoning",
                "Performance Evaluation"
            ],
            "page_summary": "This page discusses the performance of the Keye-VL model, a natural language processing model. It focuses on the model's ability to reason, with a 'Cold-start' training stage using CoT-Mix data that improves performance in both non-reasoning and reasoning modes. The model also shows improved accuracy in complex logical reasoning tasks. The introduction of the Auto-Think mode further enhances the model's ability to select reasoning modes based on difficulty. The Mix-Mode strategy in RL training synchronously improves the model's performance across various tasks.",
            "page_number": "21"
        }
    },
    "22": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {
            "8 Conclusion and Future Work": [
                "In this work, we present Kwai Keye-VL, a model that achieves leading video understanding capabilities through high-quality video data construction during both pre-training and post-training phases. The mix-mode training in the post-training phase further enables Keye-VL to respond more flexibly, resulting in superior user experience. Public benchmarks and internal human evaluations demonstrate Keye-VLs strong performance in general image-text understanding, logical reasoning, and short video applications.",
                "Nevertheless, Keye-VL has room for improvement. First, we have not specifically optimized the video encoder architecture or video encoding strategies, leaving significant potential for future enhancements. Second, Keye-VLs perceptual capabilities show only modest advantages compared to SOTA models, and its think with image ability remains preliminary, still lagging behind OpenAIs O3 model. Finally, our use of an additional MLLM as a direct reward model introduces limitations in reliability and usability due to the MLLMs inherent instruction-following capabilities and computational costs. Developing more reliable and efficient reward modeling strategies remains an open question for future research."
            ],
            "References": [
                "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024.",
                "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025.",
                "Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset, 2022.",
                "Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 35583568, 2021.",
                "Jiankang Chen, Tianke Zhang, Changyi Liu, Haojie Ding, Yaya Shi, Feng Cheng, Huihui Xiao, Bin Wen, Fan Yang, Tingting Gao, et al. Taskgalaxy: Scaling multi-modal instruction fine-tuning with tens of thousands vision task types. arXiv preprint arXiv:2502.09925, 2025.",
                "Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024a.",
                "Tao Chen, Enwei Zhang, Yuting Gao, Ke Li, Xing Sun, Yan Zhang, Hui Li, and Rongrong Ji. Mmict: Boosting multi-modal fine-tuning with in-context examples. ACM Transactions on Multimedia Computing, Communications and Applications, 2024b.",
                "Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024c.",
                "Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2418524198, 2024d.",
                "Xianfu Cheng, Wei Zhang, Shiwei Zhang, Jian Yang, Xiangyuan Guan, Xianjie Wu, Xiang Li, Ge Zhang, Jiaheng Liu, Yuying Mai, et al. Simplevqa: Multimodal factuality evaluation for multimodal large language models. arXiv preprint arXiv:2502.13059, 2025.",
                "Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 28182829, 2023.",
                "22"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "Artificial Intelligence",
                "Video Understanding",
                "Machine Learning",
                "Natural Language Processing",
                "Model Training",
                "References"
            ],
            "page_summary": "This page discusses Kwai Keye-VL, a model achieving leading video understanding capabilities through high-quality video data construction during pre-training and post-training phases. It also highlights areas for improvement such as video encoder optimization, perceptual capabilities, and reward modeling strategies.",
            "page_number": "22"
        }
    },
    "23": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {},
        "page_metadata": {
            "topics_detected": [],
            "page_summary": "",
            "page_number": "23"
        }
    },
    "24": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {},
        "page_metadata": {
            "topics_detected": [],
            "page_summary": "",
            "page_number": "24"
        }
    },
    "25": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {},
        "page_metadata": {
            "topics_detected": [],
            "page_summary": "",
            "page_number": "25"
        }
    },
    "26": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {},
        "page_metadata": {
            "topics_detected": [],
            "page_summary": "",
            "page_number": "26"
        }
    },
    "27": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {},
        "page_metadata": {
            "topics_detected": [],
            "page_summary": "",
            "page_number": "27"
        }
    },
    "28": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {},
        "page_metadata": {
            "topics_detected": [],
            "page_summary": "",
            "page_number": "28"
        }
    },
    "29": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {
            "29": {
                "9": {
                    "table_path": "/content/llm_paper_images/table-29-9.jpg",
                    "table_heading": "",
                    "table_data": " \n\nData Source MMBench MMMU AI2D MMStar MathVista OCRbench\n\nInfinity Onevision ( Gu et al. (2024)) 3459 331 902 3056 2827 826\nMMinstruct-QA ( Liu et al. (2024b)) 1349 17 0 265 0 1\nMMEvol ( Luo et al. (2024b)) 1842 140 645 1041 750 380\n\n \n\n \n\f"
                },
                "10": {
                    "table_path": "/content/llm_paper_images/table-29-10.jpg",
                    "table_heading": "",
                    "table_data": "Dataset MMBench MMMU AI2D MMStar MathVista OCRBench\n\n \n\nMM-Eureka (Meng et al. (2025)) 2,127 0 1 596 616 110\nMMPR (Wang et al. (2024b)) 10,947 0 1,186 2,009 3,143 1,044\n\f"
                }
            }
        },
        "formula": [],
        "footer": [],
        "topic_content": {
            "A Strategies for Data Decontamination": [
                "A.1 Pre-training",
                "To prevent benchmark leakage (i.e., the model memorizes evaluation examples), we conduct rigorous data decontamination strategies to avoid the data leakage problem. In pre-training, considering the large-scale data volume, we utilize simple-yet-effective pHash&minHash techniques to check our data corpus. We apply a decontamination process based on pHash and minHash. First, we utilize pHash to generate a 64-bit binary string, then convert its ones ID location list to build minHash index via 128-bit permutation hashing to group them. Next, the same buckets candidate pairs are verified using Jaccard similarity (>0.95) to identify complex variations like partial crops or local edits. Considering that some training samples have multiple images, but as long as one image is repeated with the benchmark, we will also filter out the sample from our training data. In Table 8, we have listed some open-source datasets that may have data leaks.",
                "Data Source MMBench MMMU AI2D MMStar MathVista OCRbench Infinity Onevision ( Gu et al. (2024)) 3459 331 902 3056 2827 826 MMInstruct-QA ( Liu et al. (2024b)) 1349 17 0 265 0 1 MMEvol ( Luo et al. (2024b)) 1842 140 645 1041 750 380",
                "Table 8: The number of duplicated pre-training samples with some datasets/benchmarks.",
                "A.2 Post Training",
                "In post-training, we remove any training image-question pair whose visual and textual embeddings are highly similar to instances in our evaluation benchmarks. For each candidate image-question pair in the training corpus, we compute CLIP-based cosine similarities against all evaluation benchmarks using both visual and textual encoders. Pairs exceeding similarity thresholds of 0.98 (image) and 0.50 (text) to any benchmark sample are excluded, preserving semantic diversity while systematically removing evaluation look-alikes.",
                "This decontamination protocol is applied across 29 comprehensive benchmarks spanning five critical evaluation domains: Visual Mathematics & Reasoning, General Multimodal Understanding, Chart/Dia- gram Interpretation, OCR-Centric & Document Tasks, and Robustness & Hallucination Diagnostics. The dual-threshold approach effectively safeguards evaluation integrity across all domains while maintaining the original training corpus, ensuring no meaningful reduction in model knowledge coverage.",
                "Meanwhile, Table 9 lists datasets containing a substantial number of samples exceeding the defined thresholds relative to our evaluation benchmarks. Researchers should exercise caution when employing these datasets during model training to avoid potential data leakage.",
                "Dataset MMBench MMMU AI2D MMStar MathVista OCRBench MM-Eureka (Meng et al. (2025)) 2,127 0 1 596 616 MMPR (Wang et al. (2024b)) 10,947 0 1,186 2,009 3,143 110 1,044",
                "Table 9: The number of duplicated post-training samples between various datasets."
            ],
            "B Construction of KC-MMbench": [
                "The benchmark comprises the following tasks, designed to evaluate short-form video understanding from various perspectives:",
                " Standard Product Unit (SPU): This task involves comparative analysis. We randomly sample information from different products within the same commercial category. The model is then tasked with determining whether two given pieces of product information refer to the same underlying product.",
                " Category Property Value (CPV): We collect data across various product dimensions (e.g., color, style, material). For each dimension, we construct multiple-choice questions using attributes that are semantically close, testing the models ability to make fine-grained distinctions.",
                "29"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "Data Decontamination Strategies",
                "Benchmarking",
                "Model Training",
                "Machine Learning",
                "Data Leakage Prevention"
            ],
            "page_summary": "This page discusses strategies for data decontamination to prevent benchmark leakage in machine learning model training. It includes pre-training and post-training decontamination techniques using pHash, minHash, and Jaccard similarity. The page also presents a table showing the number of duplicated pre-training samples with some datasets/benchmarks and highlights the importance of caution when employing certain datasets during model training to avoid potential data leakage.",
            "page_number": "29"
        }
    },
    "30": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {},
        "page_metadata": {
            "topics_detected": [],
            "page_summary": "",
            "page_number": "30"
        }
    },
    "31": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {
            "31": {
                "11": {
                    "table_path": "/content/llm_paper_images/table-31-11.jpg",
                    "table_heading": "",
                    "table_data": "awe Wee A Ae VE ee\n\nwe VR\n\neevee\n\n \n\nVisual Element\n\nObject Recognition\nAction Recognition\nAttribute Recognition\nSpatial Recognition\nTemporal Recognition\n\n \n\n \n\n \n\n \n\n \n\n \n\nRecognition Location Recognition\nScene/Plot Recognition\nOCR Problems\nCounting Problems\nCamera Motion Changes ae\nChanges\npe Motion Direction\nSubject Dynamic Perception Change\nUnderstanding \u2122 P Moron Speed Change\nMotion Process Per-\nception\nState Transition Perception peiep e/Color/Light\nanges\nComprehensive\nChanges (Duration)\nEvent Temporal Perception pent Time Localiza-\nion\nEvent Sequence Rear-\nrangement\nGeneral Video Specific Moment De-\nBenchmark scription\nDescriptive Video Specific Object De-\nCapability scription\nVideo Specific Sub-scene\nDescription\nVideo Dense Captioning (Generate description\nfor each event)\nSummarization of Video (Major scenes, events,\nthemes)\nAbstractive Summarization (For the entire video)\nBasic Writing (Email, Essay, Story,\nNovel)\nCopywriting / Slogan\nCreative Poetry\nCapability Brainstorming\n\nMulti-turn Conversation\n\nConversational Abil-\nity\n\na aa a ae,\n\f"
                }
            }
        },
        "formula": [],
        "footer": [],
        "topic_content": {
            "Interactional Intent": [],
            "Understanding": [],
            "Contextual Under-": [],
            "standing": [],
            "Answer Suggestion": [],
            "and Correction": [],
            "Object Grounding and": [],
            "Naming": [],
            "Knowledge-based": [
                "Common Sense Q&A",
                "Flora/Fauna,",
                "Q&A (Prior",
                "Celebrity, Food, Land-",
                "Knowledge)",
                "mark, IP Recognition"
            ],
            "General Life Knowl-": [
                "edge",
                "31"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "Object Recognition",
                "Action Recognition",
                "Attribute Recognition",
                "Spatial Recognition",
                "Temporal Recognition",
                "Visual Element",
                "Video Description",
                "Video Summarization",
                "Writing",
                "Copywriting",
                "Poetry",
                "Conversational Ability",
                "Interactional Intent",
                "Understanding",
                "Contextual Understanding",
                "Answer Suggestion",
                "Correction",
                "Object Grounding and Naming",
                "Knowledge-based Q&A",
                "General Life Knowledge"
            ],
            "page_summary": "This page discusses various recognitions in the context of visual elements and videos, including object, action, attribute, spatial, and temporal recognition. It also explores video description, summarization, writing, copywriting, and conversational ability. Additionally, it covers interactional intent, understanding, contextual understanding, answer suggestion, correction, object grounding and naming, and knowledge-based Q&A related to common sense, flora/fauna, celebrity, food, landmarks, and IP recognition.",
            "page_number": "31"
        }
    },
    "32": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {
            "32": {
                "12": {
                    "table_path": "/content/llm_paper_images/table-32-12.jpg",
                    "table_heading": "",
                    "table_data": "ontinued from previous page\nLevel 2 Level 3 Level 4\n\n \n\nMusic/Video Appreciation\nSports /Gaming Information\n\n \n\nDescriptive Reasoning\nExplanatory Reasoning\nComparative Reasoning\nPredictive Reasoning\nCounterfactual Reasoning\nSpatial Relationship Judg-\nment\n\nCausal Relationship Judg-\nment\n\nReasoning\nCapability\n\n \n\nChart and Graph Analysis\nAcademic Problem Solving\nScientific Experiment Analy-\nsis\n\nRemote Sensing Video Un-\nderstanding\n\nMedical Video Understand-\n\ning\n\nDomain-Specific\nCapability\n\n \n\nTraditional Safety\n\nSafety Instruction Attacks\n\n \n\nVideo Noise Tests (e.g., de-\ntail blur)\n\nPrompt Robustness (typos,\nunclear intent)\n\nRobustness to varied bitrate,\nresolution, clarity\nVideo-Text Irrelevance\n\nRobustness\n\n \n\f"
                }
            }
        },
        "formula": [],
        "footer": [],
        "topic_content": {},
        "page_metadata": {
            "topics_detected": [
                "technology",
                "education",
                "video analysis",
                "problem solving",
                "safety"
            ],
            "page_summary": "Page 32 focuses on level 2-4 of video appreciation, sports/gaming information, and reasoning in technology and education. It covers various types of reasoning, video noise tests, prompt robustness, and safety instructions.",
            "page_number": "32"
        }
    },
    "33": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {
                "33": {
                    "47": {
                        "image_path": "/content/llm_paper_images/figure-33-47.jpg",
                        "image_heading": "a yellow teddy bear with a white background"
                    },
                    "57": {
                        "image_path": "/content/llm_paper_images/figure-33-57.jpg",
                        "image_heading": "a heart with a small red heart on it"
                    },
                    "58": {
                        "image_path": "/content/llm_paper_images/figure-33-58.jpg",
                        "image_heading": "a blue and white abstract background"
                    },
                    "46": {
                        "image_path": "/content/llm_paper_images/figure-33-46.jpg",
                        "image_heading": "a black and white photo of a sword"
                    },
                    "65": {
                        "image_path": "/content/llm_paper_images/figure-33-65.jpg",
                        "image_heading": "a blue and white background with a white border"
                    },
                    "54": {
                        "image_path": "/content/llm_paper_images/figure-33-54.jpg",
                        "image_heading": "a cartoon lion with a big smile"
                    },
                    "44": {
                        "image_path": "/content/llm_paper_images/figure-33-44.jpg",
                        "image_heading": "a yellow teddy bear with a white background"
                    },
                    "49": {
                        "image_path": "/content/llm_paper_images/figure-33-49.jpg",
                        "image_heading": "a black and white photo of a sword"
                    },
                    "56": {
                        "image_path": "/content/llm_paper_images/figure-33-56.jpg",
                        "image_heading": "a cartoon character with a big smile"
                    },
                    "60": {
                        "image_path": "/content/llm_paper_images/figure-33-60.jpg",
                        "image_heading": "a cartoon character with a big smile"
                    },
                    "59": {
                        "image_path": "/content/llm_paper_images/figure-33-59.jpg",
                        "image_heading": "a red apple with a green leaf on top"
                    },
                    "51": {
                        "image_path": "/content/llm_paper_images/figure-33-51.jpg",
                        "image_heading": "a heart with a red heart on it"
                    },
                    "50": {
                        "image_path": "/content/llm_paper_images/figure-33-50.jpg",
                        "image_heading": "a cartoon character with a big smile"
                    },
                    "62": {
                        "image_path": "/content/llm_paper_images/figure-33-62.jpg",
                        "image_heading": "a red apple with a face on it"
                    },
                    "61": {
                        "image_path": "/content/llm_paper_images/figure-33-61.jpg",
                        "image_heading": "a heart with a red heart on it"
                    },
                    "63": {
                        "image_path": "/content/llm_paper_images/figure-33-63.jpg",
                        "image_heading": "a white and blue sky with clouds"
                    },
                    "55": {
                        "image_path": "/content/llm_paper_images/figure-33-55.jpg",
                        "image_heading": "a heart with a red heart on it"
                    },
                    "53": {
                        "image_path": "/content/llm_paper_images/figure-33-53.jpg",
                        "image_heading": "a red apple with a face and a green leaf"
                    },
                    "45": {
                        "image_path": "/content/llm_paper_images/figure-33-45.jpg",
                        "image_heading": "a yellow hay with a white background"
                    },
                    "48": {
                        "image_path": "/content/llm_paper_images/figure-33-48.jpg",
                        "image_heading": "a piece of bread with a biter on top"
                    },
                    "52": {
                        "image_path": "/content/llm_paper_images/figure-33-52.jpg",
                        "image_heading": "a blue and white abstract background"
                    },
                    "64": {
                        "image_path": "/content/llm_paper_images/figure-33-64.jpg",
                        "image_heading": "a red apple with a green leaf on it"
                    }
                }
            },
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {
            "C Case Study": [
                "C.1 Modality",
                "C.1.1 Pure Text Case",
                "&",
                "@",
                "Prompts: \"What movie does this emoji describe? \"",
                "",
                "&",
                "@",
                "X",
                "Model Responses: <think> The user now needs to determine which movie the emoji describes. First, analyze the meaning of each emoji: is a bride, is a dagger, and is a fist.",
                "Then, recall movies that involve a female protagonist in a wedding scene, armed with weapons, and fighting. For example, \"Kill Bill: Volume 1\" is a Quentin Tarantino film where the main character, Beatrix Kiddo (the Bride), is a bride at her wedding, then seeks revenge, using weapons like a dagger for combat. The elements of the bride, dagger, and fist (representing fighting) all match. Therefore, it should be this movie. </think>",
                "<answer> To determine the movie corresponding to this emoji, it is necessary to analyze the meaning of the emojis",
                "and match the plot elements: (bride): Indicates a wedding scene; (dagger): Represents a weapon; (fist): Symbolizes fighting or combat.",
                "Combining these elements, the movie \"Kill Bill: Volume 1\" fits the plot: the protagonist Beatrix Kiddo (the Bride) is a bride at her wedding, then embarks on a revenge journey, using weapons (such as a dagger) for combat, with the core elements of \"bride + weapon + fighting\" perfectly matching.",
                "Kill Bill: Volume 1 </answer>",
                "&",
                "",
                "",
                "Prompts: \"What movie does this emoji describe?",
                "",
                "@",
                "",
                "Model Responses: <think> The user now needs to determine which movie the emoji \" \" de- scribes. First, analyze each element: a woman ( ), a heart ( ), waves ( ), and a demon ( ). Recall movies with similar elements, such as \"The Shape of Water\" (The Shape of Water). In the movie, a mute female cleaner falls in love with a mysterious aquatic creature (demon-like?), and the scenes include water and emotional elements. Therefore, it is necessary to confirm the elements corresponding to each part: the female protagonist, the heart symbolizing love, the waves representing the aquatic creatures environment, and the demon-like creature. Thus, the movie is \"The Shape of Water.\"</think>",
                "",
                "Y",
                "",
                "",
                "<answer> To determine the movie corresponding to the emoji \" \", it is necessary to analyze the symbolic",
                "",
                "",
                "meaning of each element: : Represents a female character; : Symbolizes love or emotional connection;",
                "",
                "Represents water or an aquatic environment; : Can be associated with a mysterious, otherworldly creature (such as an aquatic monster). Combining these elements, the movie \"The Shape of Water\" (The Shape of Water) fits well: the story revolves around a mute female cleaner who develops a profound emotional bond with a captured aquatic creature, set against a backdrop filled with water and mysterious creatures.",
                "The Shape of Water </answer>",
                "Figure 6: An example of Kwai Keye-VL successfully deducing the corresponding movie name based on the input emojis. We directly input the emojis in text form and Kwai Keye-VL can accurately understand the meaning of emojis, which is crucial for understanding content on media platforms.",
                "33",
                "&",
                "",
                "&",
                ":"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "Artificial Intelligence",
                "Natural Language Processing",
                "Emoji",
                "Movie Understanding",
                "Case Study"
            ],
            "page_summary": "This page explores how an AI model, Kwai Keye-VL, interprets movie descriptions presented as emojis, providing a case study in AI's ability to understand and summarize content.",
            "page_number": "33"
        }
    },
    "34": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {
                "34": {
                    "66": {
                        "image_path": "/content/llm_paper_images/figure-34-66.jpg",
                        "image_heading": "a diagram of the four different lines that are connected to each other lines"
                    }
                }
            },
            "figure_captions": [
                "Prompts: Count the one-colored routes that go from D to A. Answer with a number in curly brackets e.g. 3"
            ]
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {
            "Input Image": [
                "A Cc",
                "Model Responses: <think>The current task for the user is to count the number of single-colored paths from D to A. First, it is necessary to identify the color of each path and then analyze the paths for each color separately to see if they connect D to A.",
                "First, observe the colors of the paths in the image: purple, brown, blue, orange, red, green. Then, check if the paths for each color connect D to A.",
                "Purple path: Check if the purple path connects D to A. Trace the purple path to see if it reaches A. It appears that the purple path does not connect D to A, so it is excluded.",
                "Brown path: Trace the brown path to see if it connects D to A. The brown path seems to connect D to A, so this is one. Blue path: ...",
                "Orange path: ...",
                "Red path: ...",
                "Green path: ...",
                "Now, check carefully again. Could there be other colors? For example, are there any other colors? Check the colors in the image again: purple, brown, blue, orange, red, green, and possibly others? No, the colors in the image should be these. Then, confirm again if the brown path is the only one? Or are there any others?",
                "Look carefully again, could there be other paths? For example, are there any other colors? Oh, maybe I missed something. Check again:",
                "...",
                "Oh, maybe there are no other colors that connect D to A, so the count is 1?</think>",
                "<answer>To determine the number of single-colored paths from D to A, it is necessary to analyze the connectivity of each colored path one by one:",
                "Purple path: Does not connect D to A, excluded. Brown path: Continuously connects D to A, meeting the condition. Blue path: Does not connect D to A, excluded. Orange path: Does not connect D to A, excluded. Red path: Does not connect D to A, excluded. Green path: Does not connect D to A, excluded. After analyzing each color, only the brown path meets the condition of \"single-colored path from D to A.\"",
                "1",
                "</answer>",
                "Figure 7: An example demonstrating Kwai Keye-VLs reflective ability learned from the RL process. Kwai Keye-VL checks the correctness of the results multiple times before providing the final answer, avoiding misinterpretation of complex images.",
                "34"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "image analysis",
                "path counting",
                "machine learning model responses",
                "reflection ability"
            ],
            "page_summary": "This page discusses an image analysis task, where a machine learning model is prompted to count the number of single-colored paths from D to A in a given image. The model responds by identifying the colors of the paths and analyzing their connectivity. The page highlights the model's reflective ability, as it checks the results multiple times before providing the final answer.",
            "page_number": "34"
        }
    },
    "35": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {
                "35": {
                    "67": {
                        "image_path": "/content/llm_paper_images/figure-35-67.jpg",
                        "image_heading": "a woman in a gray dress and white shirt"
                    }
                }
            },
            "figure_captions": [
                "Input Image"
            ]
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {},
        "page_metadata": {
            "topics_detected": [],
            "page_summary": "",
            "page_number": "35"
        }
    },
    "36": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {
                "36": {
                    "68": {
                        "image_path": "/content/llm_paper_images/figure-36-68.jpg",
                        "image_heading": "a col of dogs eating food from a bowl"
                    }
                }
            },
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {
            "Input Video": [
                "",
                "Prompts: You are an expert in understanding video comments. Please analyze and explain why this comment was rated as a top comment based on the video frames [video] and text. Video content information: [Video title: [Testing if Dada protects food? #Dog #DundunDada] Video ASR: [Yeah. | Already finished eating. | So cold.] Video content OCR information: [Testing Dada. Protects food or not | Eat | Give]] Comment content: [Dada: She cant bite me, right [Oh]];",
                "Model Responses: Reason Analysis: The reason this comment was rated as a top comment is due to its creativity and humor. The comment uses personification, posing the question \"She cant bite me, right\" from the perspective of the dog \"Dada,\" creating a clever contrast. In typical food protection tests, people worry about dogs biting to protect their food, but the comment reverses this thinking, making the dog worry about the owner biting it. This reversal breaks conventional thinking and creates a humorous effect. Additionally, the comment vividly showcases the cute interaction between the pet and the owner, aligning with the videos theme of testing food protection. It precisely captures the fun aspect of the video, resonating with the audience and eliciting a smile. The language is concise yet vivid, making it a top comment.",
                "Figure 9: An example demonstrating Kwai Keye-VLs understanding of short video comments: Kwai Keye-VL accurately identifies the commenter imitating a pet talking to its owner, creating a contrast and humorous effect.",
                "36"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "video comment analysis",
                "top comment analysis",
                "natural language processing",
                "artificial intelligence",
                "humor in comments",
                "contrast in comments",
                "pet-owner interaction"
            ],
            "page_summary": "This page explores the analysis of a top-rated video comment, using AI to understand the humor and contrast in the comment, aligning with the video's theme of testing food protection between pets and owners.",
            "page_number": "36"
        }
    },
    "37": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {
                "37": {
                    "69": {
                        "image_path": "/content/llm_paper_images/figure-37-69.jpg",
                        "image_heading": "a poster with a picture of a man in a suit"
                    }
                }
            },
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {
            "Input Video": [
                "",
                "Prompts: Which of the following statements best explains the observed difference in the durations of swinging, and predicts the expected effect if the middle plate were replaced with a plate made of brass? A. The middle plate stops sooner because the magnetic field exerts a direct mechanical force opposing its motion; replacing it with brass would cause it to stop even sooner due to increased magnetic interactions. B. The middle plate stops sooner due to magnetic hysteresis losses as it moves through the magnetic field; replacing it with brass would cause it to stop later because brass is less susceptible to hysteresis losses. C. The middle plate stops sooner due to eddy current damping opposing its motion; replacing it with brass, which has higher electrical resistivity, would cause it to swing shorter because larger eddy currents would be induced, leading to more damping.",
                "D. The middle plate stops sooner because the magnetic field exerts a direct mechanical force opposing its motion; replacing it with brass would cause it to stop later due to decreased magnetic interactions.",
                "E. The middle plate stops sooner due to eddy current damping opposing its motion; replacing it with brass, which has higher electrical resistivity, would cause it to swing longer because smaller eddy currents would be induced, leading to less damping.",
                "Model Responses: <think> Got it, lets try to work through this problem. First, the question is about why the middle plate stops sooner when swinging, and what happens if we replace it with brass. The options mention eddy currents, magnetic hysteresis, direct mechanical force, resistivity, etc.",
                "First, I need to remember what the setup is. The video shows a pendulum with different metal plates swinging through a magnetic field. The key here is ...",
                "The problem mentions \"the middle plate\" stops sooner. Lets assume there are ...",
                "Now, the options: Lets go through them.",
                "Option A: Direct mechanical force. No, magnetic force on the plate isnt direct mechanical; its due to induced currents. So A is out.",
                "...",
                "Option E: Eddy current damping. Correct mechanism. Middle plate stops sooner due to eddy currents. Then, brass has higher resistivity than, say, copper. Higher resistivity means smaller eddy currents (since eddy current is proportional to 1/resistivity, other factors constant). So smaller eddy currents mean less damping, so the plate swings longer. That matches E.",
                "Let me confirm: The induced current depends on the resistance of the material. For a given induced emf, current I = emf/R. So if R is higher, then I is smaller. ... Thats option E.</think>",
                "<answer>To determine the best explanation for the observed difference in swinging durations and the effect of replacing the middle plate with brass, we analyze the phenomenon using principles of electromagnetism, specifically eddy current damping:",
                "### **Key Observations and Analysis**",
                "The setup involves metal plates ...",
                "### **Evaluation of Options**",
                "- **Option A**: Incorrect...",
                "...",
                "- **Option E**: Correct. The middle plate stops sooner due to eddy current damping (larger eddy currents in lower-resistivity material). Brass has higher electrical resistivity than the original material (e.g., copper or aluminum), so smaller eddy currents are induced, reducing damping. This results in the brass plate swinging longer.",
                "**Final Answer** E </answer>",
                "Figure 10: An example demonstrating how Kwai Keye-VL solves complex physics problems based on video content. Kwai Keye-VL accurately extracts the objects and setup from the video, analyzes each option using physics knowledge, checks them one by one, and finally confirms the correct answer.",
                "37"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "Physics",
                "Electromagnetism",
                "Eddy Currents",
                "Material Science"
            ],
            "page_summary": "This page discusses an analysis of a physics experiment involving a pendulum with different metal plates moving through a magnetic field. The focus is on identifying why the middle plate stops sooner and the expected effect if it were replaced with a brass plate. The principles of electromagnetism, specifically eddy current damping, are applied to evaluate the options.",
            "page_number": "37"
        }
    },
    "38": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {
                "38": {
                    "70": {
                        "image_path": "/content/llm_paper_images/figure-38-70.jpg",
                        "image_heading": "a bunch of fruit and vegetables in a container"
                    }
                }
            },
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {
            "import os import random from PIL import Image": [
                "",
                "\\# Load the image image_path = \"test.webp\" img = Image.open(image_path)",
                "\\# Define the coordinates for the strawberry region x1, y1, x2, y2 = 350, 150, 500, 300",
                "\\# Crop the image cropped_img = img.crop((x1, y1, x2, y2))",
                "\\# Apply a zoom effect by resizing the cropped image zoom_factor = 2",
                "zoomed_img = cropped_img.resize((cropped_img.width * zoom_factor, cropped_img.height * zoom_factor))",
                "\\# Save the processed image with a random suffix random_suffix = random.randint(1000, 9999) processed_filename = f\"test_{random_suffix}.webp\" processed_path = \"/mnt/data/temp_processed_images/\" + processed_filename",
                "zoomed_img.save(processed_path)",
                "\\# Print the path of the saved cropped image print(processed_path)",
                "<sandbox_output> <subimage_strawberry> </sandbox_output> By carefully examining the cropped and enlarged image, it is clear that the number of strawberries is 20. The enlarged image shows the strawberries more clearly, with no overlapping or blurring, making the count more accurate. </think>",
                "<answer> The number of strawberries in the picture is 20 . </answer>",
                "Figure 11: An example of Kwai Keye-VL writing code to extract the relevant parts from high-resolution images and successfully solves the counting problem. In the agentic thinking mode, Kwai Keye-VL can independently write code to edit input images or call tools.",
                "38"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "image processing",
                "counting objects in image",
                "Python programming",
                "PIL library"
            ],
            "page_summary": "This page discusses the process of counting objects in a high-resolution image using Python and the PIL library. The image is first cropped and zoomed in to clearly show the objects, which in this case are strawberries. The final count of strawberries in the image is 20.",
            "page_number": "38"
        }
    },
    "39": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {
                "39": {
                    "71": {
                        "image_path": "/content/llm_paper_images/figure-39-71.jpg",
                        "image_heading": "a triangle with the sides of two triangles"
                    }
                }
            },
            "figure_captions": [
                "Input Image"
            ]
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {
            "No-Thinking Case": [
                "X",
                "Prompts: Find x.",
                "Model Responses: <analysis>This question requires finding the value of x using the Pythagorean theo- rem, which is a straightforward calculation given the right triangle. Therefore, /no_think mode is more appropriate.</analysis>",
                "The given image depicts a right triangle with one leg measuring 2 units, the hypotenuse measuring 5 units, and the other leg labeled as x. To find the value of x, we can use the Pythagorean theorem, which states that in a right triangle, the square of the length of the hypotenuse (c) is equal to the sum of the squares of the lengths of the other two sides (a and b):",
                "c2 = a2 + b2",
                "In this case, the hypotenuse c is 5, one leg a is 2, and the other leg b is x. Plugging these values into the theorem, we get:",
                "52 = 22 + x2",
                "Simplifying the equation:",
                "25 = 4 + x2",
                "Subtract 4 from both sides to isolate x2:",
                "21 = x2",
                "Taking the square root of both sides to solve for x:",
                "",
                "",
                "x = 21",
                "Thus, the value of x is 21.",
                "Figure 12: An example demonstrating Kwai Keye-VL analyzing the difficulty of a problem and choosing the Non-Thinking mode to solve a simple geometry problem.",
                "39"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "mathematics",
                "geometry",
                "right triangle",
                "Pythagorean theorem"
            ],
            "page_summary": "This page discusses a geometry problem involving a right triangle and the Pythagorean theorem to find the length of a side. The problem is solved using the No-Thinking mode of Kwai Keye-VL, an AI model.",
            "page_number": "39"
        }
    },
    "40": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {
                "40": {
                    "72": {
                        "image_path": "/content/llm_paper_images/figure-40-72.jpg",
                        "image_heading": "a circle with a center and a center at the end"
                    }
                }
            },
            "figure_captions": [
                "Input Image"
            ]
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {
            "Thinking Case": [
                "A B",
                "Prompts: Find x.",
                "Model Responses: <analysis> This question involves calculating the area of a shaded region formed by two semicircles, where specific geometric properties and relationships between the semicircles and the chord are given. This requires understanding the geometry and performing calculations to determine the area. Therefore, /think mode is more appropriate.</analysis>",
                "<think>The problem that users need to solve now is the area of the shaded region. First, it is necessary to understand the composition of the figure: the shaded region should be the area of the larger semicircle minus the area of the smaller semicircle, or the area of the larger semicircle minus the area of the smaller semicircle plus the area of the smaller semicircle? No, looking at the figure, the shaded region is the part of the larger semicircle excluding the smaller semicircle, so it is the area of the larger semicircle minus the area of the smaller semicircle. Then, it is necessary to find the radii of the larger and smaller semicircles.",
                "First, let the radius of the larger semicircle be R and the radius of the smaller semicircle be r. Since CD is parallel to AB and CD touches the smaller semicircle, the distance from the center O of the larger semicircle to CD is equal to the radius r of the smaller semicircle. The length of CD is 4, so in the right triangle formed by half of CD, the radius R of the larger semicircle, and the distance r, according to the Pythagorean theorem, we have (CD/2)2 + r2 = R2, which is 22 + r2 = R2, i.e., 4 + r2 = R2.",
                "Then, the area of the shaded region is the area of the larger semicircle minus the area of the smaller semicircle, which is 1 2R2  1 2r2 = 1 2(R2  r2). From the previous equation, R2  r2 = 4, so the area is 1 of the shaded region is 2.</think> 2  4 = 2. Thus, the area",
                "<answer> To solve this problem, we can analyze the area of the shaded region by combining the Pythagorean theorem and the formula for the area of a circle.",
                "Step 1: Define Variables and Analyze Geometric Relationships Let the radius of the larger semicircle be ( R ) and the radius of the smaller semicircle be ( r ). Observing the figure, we know that the chord ( CD ) is parallel to the diameter ( AB ) of the larger semicircle and is tangent to the smaller semicircle. Therefore, the distance from the center ( O ) of the larger semicircle to ( CD ) is equal to the radius ( r ) of the smaller semicircle.",
                "The length of ( CD ) is 4, so half of ( CD ) is ( CD",
                "2 = 2 ). At this point, a right triangle can be formed: one leg is (",
                "CD 2 = 2 ), the other leg is the distance from the center ( O ) to ( CD ) (i.e., ( r )), and the hypotenuse is the radius ( R ) of the larger semicircle.",
                "Step 2: Use the Pythagorean Theorem to Find ( R2  r2 ) According to the Pythagorean theorem, in the right triangle,",
                "Step 2: Use the Pythagorean Theorem to Find ( R?  r? ) According to the Pythagorean theorem, in the right triangle, we have: [ (P) +7 = R? ] Substituting ( cp = 2), we get: [24+P=R = 44 =R = R-P7r=4] Step 3: Calculate the Area of the Shaded Region The area of the shaded region is equal to the area of the larger semicircle minus the area of the smaller semicircle. According to the formula for the area of a circle (S = mr ), the area of the semicircle is ( har ). Therefore: [ Area of the shaded region = 37R?  har = 370(R? _ r) ] Substituting (R*  7? = 4), we get: [ Area of the shaded region = 4m x4= 27] In conclusion, the area of the shaded region is (| 27 |). </answer>",
                "Figure 13: An example demonstrating Kwai Keye-VL choosing the Thinking mode to solve a hard geometry problem.",
                "40"
            ]
        },
        "page_metadata": {
            "topics_detected": [
                "geometry",
                "problem solving",
                "circle",
                "semicircle",
                "shaded region",
                "Pythagorean theorem"
            ],
            "page_summary": "This page focuses on solving a hard geometry problem using Thinking mode. The problem involves finding the area of a shaded region formed by two semicircles with specific geometric properties. The solution combines the Pythagorean theorem and the formula for the area of a circle to calculate the area of the shaded region.",
            "page_number": "40"
        }
    },
    "41": {
        "header": [],
        "title": [],
        "narrative_text": [],
        "list_item": [],
        "image_data": {
            "images": {},
            "figure_captions": []
        },
        "table": {},
        "formula": [],
        "footer": [],
        "topic_content": {},
        "page_metadata": {
            "topics_detected": [],
            "page_summary": "",
            "page_number": "41"
        }
    }
}